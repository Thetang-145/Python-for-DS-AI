{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Modify the Decision Tree scratch code in our lecture such that:\r\n",
    "- Modify the scratch code so it can accept an hyperparameter <code>max_depth</code>, in which it will continue create the tree until max_depth is reached.</li>\r\n",
    "- Put everything into a class <code>DecisionTree</code>.  It should have at least two methods, <code>fit()</code>, and <code>predict()</code>\r\n",
    "- Load the iris data and try with your class</li>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "class Node:\r\n",
    "    def __init__(self, predicted_class):\r\n",
    "        self.predicted_class = predicted_class\r\n",
    "        self.feature_index = 0\r\n",
    "        self.threshold = 0\r\n",
    "        self.left = None\r\n",
    "        self.right = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "class DecisionTree:\r\n",
    "    def __init__(self , max_depth=5):\r\n",
    "        self.max_depth = max_depth\r\n",
    "\r\n",
    "    def fit(self, Xtrain, ytrain, n_classes, depth=0):\r\n",
    "        num_samples_per_class = [np.sum(ytrain == i) for i in range(n_classes)]\r\n",
    "        #predicted class using the majority of sample class\r\n",
    "        predicted_class = np.argmax(num_samples_per_class)\r\n",
    "        \r\n",
    "        #define the parent node\r\n",
    "        self.node = Node(predicted_class = predicted_class)\r\n",
    "        #perform recursion\r\n",
    "        if depth < self.max_depth:\r\n",
    "            feature, threshold = self.find_split(Xtrain, ytrain, n_classes)\r\n",
    "            if feature is not None:\r\n",
    "                #take all the indices that is less than threshold\r\n",
    "                indices_left = Xtrain[:, feature] < threshold\r\n",
    "                X_left, y_left = Xtrain[indices_left], ytrain[indices_left]\r\n",
    "                #tilde for negation\r\n",
    "                X_right, y_right = Xtrain[~indices_left], ytrain[~indices_left]\r\n",
    "                #take note for later decision\r\n",
    "                self.node.feature_index = feature\r\n",
    "                self.node.threshold = threshold\r\n",
    "                self.node.left = self.fit(X_left, y_left, n_classes, depth + 1)\r\n",
    "                self.node.right = self.fit(X_right, y_right, n_classes, depth + 1)\r\n",
    "                print(type(self.node.left))\r\n",
    "        return self.node\r\n",
    "    \r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "    def find_split(self, X, y, n_classes):\r\n",
    "        \"\"\" Find split where children has lowest impurity possible\r\n",
    "        in condition where the purity should also be less than the parent,\r\n",
    "        if not, stop.\r\n",
    "        \"\"\"\r\n",
    "        n_samples, n_features = X.shape\r\n",
    "        if n_samples <= 1:\r\n",
    "            return None, None\r\n",
    "        \r\n",
    "        #so it will not have any warning about \"referenced before assignments\"\r\n",
    "        feature_ix, threshold = None, None\r\n",
    "        \r\n",
    "        # Count of each class in the current node.\r\n",
    "        sample_per_class_parent = [np.sum(y == c) for c in range(n_classes)] #[2, 2]\r\n",
    "        \r\n",
    "        # Gini of parent node.\r\n",
    "        best_gini = 1.0 - sum((n / n_samples) ** 2 for n in sample_per_class_parent)\r\n",
    "\r\n",
    "        # Loop through all features.\r\n",
    "        for feature in range(n_features):\r\n",
    "            \r\n",
    "            # Sort data along selected feature.\r\n",
    "            sample_sorted = sorted(X[:, feature]) #[2, 3, 10, 19]\r\n",
    "            sort_idx = np.argsort(X[:, feature])\r\n",
    "            y_sorted = y[sort_idx] #[0, 0, 1, 1]\r\n",
    "                    \r\n",
    "            sample_per_class_left = [0] * n_classes   #[0, 0]\r\n",
    "            \r\n",
    "            sample_per_class_right = sample_per_class_parent.copy() #[2, 2]\r\n",
    "\r\n",
    "            #sample_sorted, y_sorted = zip(*sorted(zip(X[:, i], y)))\r\n",
    "            #loop through each threshold, 2.5, 6.5, 14.5\r\n",
    "            #1st iter: [-] [-++]\r\n",
    "            #2nd iter: [--] [++]\r\n",
    "            #3rd iter: [--+] [+]\r\n",
    "            for i in range(1, n_samples): #1 to 3 (excluding 4)\r\n",
    "                #the class of that sample\r\n",
    "                c = y_sorted[i - 1]  #[0]\r\n",
    "                \r\n",
    "                #put the sample to the left\r\n",
    "                sample_per_class_left[c] += 1  #[1, 0]\r\n",
    "                            \r\n",
    "                #take the sample out from the right  [1, 2]\r\n",
    "                sample_per_class_right[c] -= 1\r\n",
    "                \r\n",
    "                gini_left = 1.0 - sum(\r\n",
    "                    (sample_per_class_left[x] / i) ** 2 for x in range(n_classes)\r\n",
    "                )\r\n",
    "                            \r\n",
    "                #we divided by n_samples - i since we know that the left amount of samples\r\n",
    "                #since left side has already i samples\r\n",
    "                gini_right = 1.0 - sum(\r\n",
    "                    (sample_per_class_right[x] / (n_samples - i)) ** 2 for x in range(n_classes)\r\n",
    "                )\r\n",
    "\r\n",
    "                #weighted gini\r\n",
    "                weighted_gini = ((i / n_samples) * gini_left) + ( (n_samples - i) /n_samples) * gini_right\r\n",
    "\r\n",
    "                # in case the value are the same, we do not split\r\n",
    "                # (both have to end up on the same side of a split).\r\n",
    "                if sample_sorted[i] == sample_sorted[i - 1]:\r\n",
    "                    continue\r\n",
    "\r\n",
    "                if weighted_gini < best_gini:\r\n",
    "                    best_gini = weighted_gini\r\n",
    "                    feature_ix = feature\r\n",
    "                    threshold = (sample_sorted[i] + sample_sorted[i - 1]) / 2  # midpoint\r\n",
    "\r\n",
    "        #return the feature number and threshold \r\n",
    "        #used to find best split\r\n",
    "        return feature_ix, threshold\r\n",
    "\r\n",
    "    def predict(self, sample):\r\n",
    "        tree = self.node\r\n",
    "        for sample_i in sample:\r\n",
    "            while tree.left:\r\n",
    "                if sample_i[tree.feature_index] < tree.threshold:\r\n",
    "                    tree = tree.left\r\n",
    "                else:\r\n",
    "                    tree = tree.right\r\n",
    "        return tree.predicted_class"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "source": [
    "class DecisionTree:\r\n",
    "    def __init__(self , max_depth=5):\r\n",
    "        self.max_depth = max_depth\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.n_classes_ = len(set(y))\r\n",
    "        self.n_features_ = X.shape[1]\r\n",
    "        self.node = self.grow_tree(X, y, self.n_classes_)\r\n",
    "\r\n",
    "    def grow_tree(self, Xtrain, ytrain, n_classes, depth=0):\r\n",
    "        num_samples_per_class = [np.sum(ytrain == i) for i in range(n_classes)]\r\n",
    "        #predicted class using the majority of sample class\r\n",
    "        predicted_class = np.argmax(num_samples_per_class)\r\n",
    "        \r\n",
    "        #define the parent node\r\n",
    "        node = Node(predicted_class = predicted_class)\r\n",
    "        #perform recursion\r\n",
    "        if depth < self.max_depth:\r\n",
    "            feature, threshold = self.find_split(Xtrain, ytrain, n_classes)\r\n",
    "            if feature is not None:\r\n",
    "                #take all the indices that is less than threshold\r\n",
    "                indices_left = Xtrain[:, feature] < threshold\r\n",
    "                X_left, y_left = Xtrain[indices_left], ytrain[indices_left]\r\n",
    "                #tilde for negation\r\n",
    "                X_right, y_right = Xtrain[~indices_left], ytrain[~indices_left]\r\n",
    "                #take note for later decision\r\n",
    "                node.feature_index = feature\r\n",
    "                node.threshold = threshold\r\n",
    "                node.left = self.grow_tree(X_left, y_left, n_classes, depth + 1)\r\n",
    "                node.right = self.grow_tree(X_right, y_right, n_classes, depth + 1)\r\n",
    "        return node\r\n",
    "    \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def find_split(self, X, y, n_classes):\r\n",
    "        \"\"\" Find split where children has lowest impurity possible\r\n",
    "        in condition where the purity should also be less than the parent,\r\n",
    "        if not, stop.\r\n",
    "        \"\"\"\r\n",
    "        n_samples, n_features = X.shape\r\n",
    "        if n_samples <= 1:\r\n",
    "            return None, None\r\n",
    "        \r\n",
    "        #so it will not have any warning about \"referenced before assignments\"\r\n",
    "        feature_ix, threshold = None, None\r\n",
    "        \r\n",
    "        # Count of each class in the current node.\r\n",
    "        sample_per_class_parent = [np.sum(y == c) for c in range(n_classes)] #[2, 2]\r\n",
    "        \r\n",
    "        # Gini of parent node.\r\n",
    "        best_gini = 1.0 - sum((n / n_samples) ** 2 for n in sample_per_class_parent)\r\n",
    "\r\n",
    "        # Loop through all features.\r\n",
    "        for feature in range(n_features):\r\n",
    "            \r\n",
    "            # Sort data along selected feature.\r\n",
    "            sample_sorted = sorted(X[:, feature]) #[2, 3, 10, 19]\r\n",
    "            sort_idx = np.argsort(X[:, feature])\r\n",
    "            y_sorted = y[sort_idx] #[0, 0, 1, 1]\r\n",
    "                    \r\n",
    "            sample_per_class_left = [0] * n_classes   #[0, 0]\r\n",
    "            \r\n",
    "            sample_per_class_right = sample_per_class_parent.copy() #[2, 2]\r\n",
    "\r\n",
    "            #sample_sorted, y_sorted = zip(*sorted(zip(X[:, i], y)))\r\n",
    "            #loop through each threshold, 2.5, 6.5, 14.5\r\n",
    "            #1st iter: [-] [-++]\r\n",
    "            #2nd iter: [--] [++]\r\n",
    "            #3rd iter: [--+] [+]\r\n",
    "            for i in range(1, n_samples): #1 to 3 (excluding 4)\r\n",
    "                #the class of that sample\r\n",
    "                c = y_sorted[i - 1]  #[0]\r\n",
    "                \r\n",
    "                #put the sample to the left\r\n",
    "                sample_per_class_left[c] += 1  #[1, 0]\r\n",
    "                            \r\n",
    "                #take the sample out from the right  [1, 2]\r\n",
    "                sample_per_class_right[c] -= 1\r\n",
    "                \r\n",
    "                gini_left = 1.0 - sum(\r\n",
    "                    (sample_per_class_left[x] / i) ** 2 for x in range(n_classes)\r\n",
    "                )\r\n",
    "                            \r\n",
    "                #we divided by n_samples - i since we know that the left amount of samples\r\n",
    "                #since left side has already i samples\r\n",
    "                gini_right = 1.0 - sum(\r\n",
    "                    (sample_per_class_right[x] / (n_samples - i)) ** 2 for x in range(n_classes)\r\n",
    "                )\r\n",
    "\r\n",
    "                #weighted gini\r\n",
    "                weighted_gini = ((i / n_samples) * gini_left) + ( (n_samples - i) /n_samples) * gini_right\r\n",
    "\r\n",
    "                # in case the value are the same, we do not split\r\n",
    "                # (both have to end up on the same side of a split).\r\n",
    "                if sample_sorted[i] == sample_sorted[i - 1]:\r\n",
    "                    continue\r\n",
    "\r\n",
    "                if weighted_gini < best_gini:\r\n",
    "                    best_gini = weighted_gini\r\n",
    "                    feature_ix = feature\r\n",
    "                    threshold = (sample_sorted[i] + sample_sorted[i - 1]) / 2  # midpoint\r\n",
    "\r\n",
    "        #return the feature number and threshold \r\n",
    "        #used to find best split\r\n",
    "        return feature_ix, threshold\r\n",
    "\r\n",
    "    def predict(self, sample):\r\n",
    "        tree = self.node\r\n",
    "        for sample_i in sample:\r\n",
    "            while tree.left:\r\n",
    "                if sample_i[tree.feature_index] < tree.threshold:\r\n",
    "                    tree = tree.left\r\n",
    "                else:\r\n",
    "                    tree = tree.right\r\n",
    "        return tree.predicted_class"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "source": [
    "from sklearn.datasets import load_iris\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "dataset = load_iris()\r\n",
    "X, y = dataset.data, dataset.target\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
    "                test_size=0.3, shuffle=True, random_state=42)\r\n",
    "\r\n",
    "model = DecisionTree(max_depth=10)\r\n",
    "model.fit(X_train, y_train)\r\n",
    "pred = [model.predict([x]) for x in X_test]\r\n",
    "print(pred)\r\n",
    "print(y_test)\r\n",
    "print(classification_report(y_test, pred))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 2, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 2, 1, 0, 0]\n",
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      0.85      0.92        13\n",
      "           2       0.87      1.00      0.93        13\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}