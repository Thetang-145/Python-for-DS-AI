{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Modify the Gradient Boosting scratch code in our lecture such that:\n",
    "- Notice that we are still using max_depth = 1.  Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for binary classification.  Load the breast cancer data from sklearn and see that it works.\n",
    "- Further change the code so that it works for multiclass classification.  Load the digits data from sklearn and see that it works\n",
    "- Put everything into class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "from scipy.special import expit\r\n",
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.dummy import DummyRegressor\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.datasets import load_digits\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "class GradientBoosting:\r\n",
    "    def __init__(self, S=5, learning_rate=1, max_depth = 1, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4):\r\n",
    "        self.S = S\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.max_depth = max_depth\r\n",
    "        self.min_samples_split = min_samples_split\r\n",
    "        self.regression=regression\r\n",
    "            \r\n",
    "        #initialize regression trees\r\n",
    "        tree_params = {'max_depth': self.max_depth,\r\n",
    "                      'min_samples_split': self.min_samples_split}\r\n",
    "        self.models = [DecisionTreeRegressor(**tree_params) for _ in range(S)]        \r\n",
    "        first_model = DummyRegressor(strategy='mean')\r\n",
    "        self.models.insert(0, first_model)\r\n",
    "        \r\n",
    "    def grad(self, y, h):\r\n",
    "        return y - h\r\n",
    "    \r\n",
    "    def fit(self, X, y):  #<----X_train\r\n",
    "        \r\n",
    "        #fit the first model\r\n",
    "        self.models[0].fit(X, y)\r\n",
    "        \r\n",
    "        for i in range(self.S):\r\n",
    "            #predict\r\n",
    "            yhat = self.predict(X, self.models[:i+1], with_argmax=False)\r\n",
    "            \r\n",
    "            #get the gradient\r\n",
    "            gradient = self.grad(y, yhat)\r\n",
    "            \r\n",
    "            #fit the next model with gradient\r\n",
    "            self.models[i+1].fit(X, gradient)\r\n",
    "    \r\n",
    "    def predict(self, X, models=None, with_argmax=True):\r\n",
    "        if models is None:\r\n",
    "            models = self.models\r\n",
    "        h0 = models[0].predict(X)  #first use the dummy model\r\n",
    "        boosting = sum(self.learning_rate * model.predict(X) for model in models[1:])\r\n",
    "        yhat = h0 + boosting\r\n",
    "        \r\n",
    "        if not self.regression:\r\n",
    "            #turn into probability using softmax\r\n",
    "            print(models[0], X.shape, yhat.shape, yhat[0])\r\n",
    "            yhat = np.exp(yhat) / np.sum(np.exp(yhat), axis=1, keepdims=True)\r\n",
    "            if with_argmax:\r\n",
    "                yhat = np.argmax(yhat, axis=1)\r\n",
    "        return yhat\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Regression\r\n",
    "\r\n",
    "from sklearn.datasets import load_boston\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "X, y = load_boston(return_X_y=True)\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
    "                        test_size=0.3, random_state=42)\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4)\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "\r\n",
    "#print metrics\r\n",
    "print(\"MSE: \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "n_estimators = 200\r\n",
    "\r\n",
    "#=====SKlearn========\r\n",
    "#Compare to sklearn: ls is the same as our mse\r\n",
    "sklearn_model = GradientBoostingRegressor(\r\n",
    "    n_estimators=n_estimators,\r\n",
    "    learning_rate = 0.1,\r\n",
    "    max_depth=3,\r\n",
    "    loss='ls'\r\n",
    ")\r\n",
    "\r\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\r\n",
    "print(\"Sklearn MSE: \", mean_squared_error(y_test, yhat_sk))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE:  7.559246368428935\n",
      "Sklearn MSE:  7.7954727382926094\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# Binary classification\r\n",
    "\r\n",
    "from sklearn.datasets import load_breast_cancer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "\r\n",
    "X, y = load_breast_cancer(return_X_y=True)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = \\\r\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\r\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\r\n",
    "for each_class in range(len(set(y))):\r\n",
    "    cond = y_train==each_class\r\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=False)\r\n",
    "print(y_train_encoded)\r\n",
    "model.fit(X_train, y_train_encoded)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "\r\n",
    "# #print metrics\r\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\r\n",
    "\r\n",
    "#=====SKlearn========\r\n",
    "#Compare to sklearn: ls is the same as our accuracy\r\n",
    "sklearn_model = GradientBoostingClassifier(\r\n",
    "    n_estimators=n_estimators,\r\n",
    "    learning_rate = 0.1,\r\n",
    "    max_depth=1\r\n",
    ")\r\n",
    "\r\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\r\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.37437186 0.62562814]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.33194201 0.66805799]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.29158827 0.70841173]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.25318215 0.74681785]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.21690606 0.78309394]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.18134432 0.81865568]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.14741263 0.85258737]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.11543045 0.88456955]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.08391977 0.91608023]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.05454978 0.94545022]\n",
      "DummyRegressor() (398, 30) (398, 2) [0.02732766 0.97267234]\n",
      "DummyRegressor() (398, 30) (398, 2) [5.38980491e-04 9.99461020e-01]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.02546011  1.02546011]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.05138738  1.05138738]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.07545614  1.07545614]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.09868334  1.09868334]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.12103705  1.12103705]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.14242252  1.14242252]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.16306284  1.16306284]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.18323297  1.18323297]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.20255079  1.20255079]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.22145037  1.22145037]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.23956808  1.23956808]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.2573254  1.2573254]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.2743732  1.2743732]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.29209461  1.29209461]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.30839755  1.30839755]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.3251759  1.3251759]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.33980468  1.33980468]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.35484439  1.35484439]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.36947792  1.36947792]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.3835763  1.3835763]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.39787495  1.39787495]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.41127414  1.41127414]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.42495193  1.42495193]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.43790249  1.43790249]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.45038056  1.45038056]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.46315561  1.46315561]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.47568069  1.47568069]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.48836265  1.48836265]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.49997636  1.49997636]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.5117141  1.5117141]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.52364146  1.52364146]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.53514114  1.53514114]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.54585354  1.54585354]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.55690584  1.55690584]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.56752723  1.56752723]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.57836328  1.57836328]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.58817716  1.58817716]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.59794299  1.59794299]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.60804587  1.60804587]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.61775399  1.61775399]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.627686  1.627686]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.63705462  1.63705462]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.64665877  1.64665877]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.65550386  1.65550386]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.66465856  1.66465856]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.6738011  1.6738011]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.68209651  1.68209651]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.69082575  1.69082575]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.69919808  1.69919808]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.7075052  1.7075052]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.71596993  1.71596993]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.72379714  1.72379714]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.73182894  1.73182894]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.73931636  1.73931636]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.74721706  1.74721706]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.75460564  1.75460564]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.76238681  1.76238681]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.76996225  1.76996225]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.77694083  1.77694083]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.78431197  1.78431197]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.79144778  1.79144778]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.79801346  1.79801346]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.80452493  1.80452493]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.81140738  1.81140738]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.81800536  1.81800536]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.82453365  1.82453365]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.83104914  1.83104914]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.83761192  1.83761192]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.84387041  1.84387041]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.85089871  1.85089871]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.85705181  1.85705181]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.86312081  1.86312081]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.86909438  1.86909438]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.87552585  1.87552585]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.88159593  1.88159593]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.88780895  1.88780895]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.89353566  1.89353566]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.89963128  1.89963128]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.90523426  1.90523426]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.91078533  1.91078533]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.91627099  1.91627099]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.92169047  1.92169047]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.92747282  1.92747282]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.93283171  1.93283171]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.93812717  1.93812717]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.94335269  1.94335269]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.94853486  1.94853486]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.95400282  1.95400282]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.95901882  1.95901882]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.96442824  1.96442824]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.9694316  1.9694316]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.97436385  1.97436385]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.97925556  1.97925556]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.98409868  1.98409868]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.98889512  1.98889512]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.9938817  1.9938817]\n",
      "DummyRegressor() (398, 30) (398, 2) [-0.998606  1.998606]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.00322493  2.00322493]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.00821108  2.00821108]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.01280592  2.01280592]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.01735689  2.01735689]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.02207584  2.02207584]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.02655516  2.02655516]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.03100166  2.03100166]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.03542762  2.03542762]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.03974138  2.03974138]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.04440381  2.04440381]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.04871714  2.04871714]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.05297375  2.05297375]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.05718914  2.05718914]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.06138159  2.06138159]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.06573398  2.06573398]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.06986107  2.06986107]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.0739631  2.0739631]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.07830008  2.07830008]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.08234335  2.08234335]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.08630708  2.08630708]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.09059148  2.09059148]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.0945574  2.0945574]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.09847218  2.09847218]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.10236597  2.10236597]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.10652668  2.10652668]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.11009183  2.11009183]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.11412363  2.11412363]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.11806974  2.11806974]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.12182268  2.12182268]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.12555098  2.12555098]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.1292016  2.1292016]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.13307668  2.13307668]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.13689224  2.13689224]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.14054172  2.14054172]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.14414476  2.14414476]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.14771665  2.14771665]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.15127077  2.15127077]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.15479558  2.15479558]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.15858348  2.15858348]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.16222618  2.16222618]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.16565078  2.16565078]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.16910791  2.16910791]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.17252067  2.17252067]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.17610767  2.17610767]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.17948411  2.17948411]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.18282696  2.18282696]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.18628553  2.18628553]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.189615  2.189615]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.1929045  2.1929045]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.1961752  2.1961752]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.19969481  2.19969481]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.20292794  2.20292794]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.2062806  2.2062806]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.20947252  2.20947252]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.21264851  2.21264851]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.21598288  2.21598288]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.21926313  2.21926313]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.22235853  2.22235853]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.22543068  2.22543068]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.22894225  2.22894225]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.23202466  2.23202466]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.23506337  2.23506337]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.2382047  2.2382047]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.24118502  2.24118502]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.24417479  2.24417479]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.24733111  2.24733111]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.25040897  2.25040897]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.2533816  2.2533816]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.25671997  2.25671997]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.25964388  2.25964388]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.26251181  2.26251181]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.26540906  2.26540906]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.2682398  2.2682398]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.27133426  2.27133426]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.27401414  2.27401414]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.27695742  2.27695742]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.27973505  2.27973505]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.28254  2.28254]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.28532607  2.28532607]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.28832069  2.28832069]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.29107945  2.29107945]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.2938199  2.2938199]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.29676252  2.29676252]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.29957015  2.29957015]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.30223591  2.30223591]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.30493054  2.30493054]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.30760303  2.30760303]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.31047651  2.31047651]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.31310883  2.31310883]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.31574253  2.31574253]\n",
      "DummyRegressor() (398, 30) (398, 2) [-1.31832078  2.31832078]\n",
      "DummyRegressor() (171, 30) (171, 2) [-1.32130988  2.32130988]\n",
      "Our accuracy:  0.9649122807017544\n",
      "Sklearn accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Multiclass classification\r\n",
    "\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "\r\n",
    "X, y = load_digits(return_X_y=True)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = \\\r\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\r\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\r\n",
    "for each_class in range(len(set(y))):\r\n",
    "    cond = y_train==each_class\r\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=False)\r\n",
    "model.fit(X_train, y_train_encoded)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "\r\n",
    "# #print metrics\r\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\r\n",
    "\r\n",
    "#=====SKlearn========\r\n",
    "#Compare to sklearn: ls is the same as our accuracy\r\n",
    "sklearn_model = GradientBoostingClassifier(\r\n",
    "    n_estimators=n_estimators,\r\n",
    "    learning_rate = 0.1,\r\n",
    "    max_depth=1\r\n",
    ")\r\n",
    "\r\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\r\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our accuracy:  0.9314814814814815\n",
      "Sklearn accuracy:  0.9481481481481482\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "yhat = model.predict(X_test)\r\n",
    "yhat_am = model.predict(X_test, with_argmax=False)\r\n",
    "print(np.unique(y).shape)\r\n",
    "print(yhat[:5])\r\n",
    "print(yhat_am[0].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(10,)\n",
      "[6 9 3 7 2]\n",
      "(10,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}