{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Library import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make/Load dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import make_classification\r\n",
    "X, y = make_classification(n_samples=500, n_features=10, n_informative=4,\r\n",
    "                             n_clusters_per_class=2, random_state=14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import load_boston\r\n",
    "boston = load_boston()\r\n",
    "X = boston.data\r\n",
    "y = boston.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.datasets import load_iris\r\n",
    "iris = load_iris()\r\n",
    "X = iris.data[:, 2:]\r\n",
    "y = iris.target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standardize/Normallize Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "scaler = StandardScaler()\r\n",
    "X = scaler.fit_transform(X)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insert interception"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "intercept = np.ones((X_train.shape[0], 1))\r\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classification report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### From scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class classification_report_fromSratch:\r\n",
    "    def __init__(self, y_actual, y_predict):\r\n",
    "        self.y_actual = y_actual\r\n",
    "        self.y_predict = y_predict\r\n",
    "        self.TP = sum((self.y_actual == 1) & (self.y_predict == 1))\r\n",
    "        self.FN = sum((self.y_actual == 1) & (self.y_predict == 0))\r\n",
    "        self.FP = sum((self.y_actual == 0) & (self.y_predict == 1))\r\n",
    "        self.TN = sum((self.y_actual == 0) & (self.y_predict == 0))\r\n",
    "\r\n",
    "    def accuracy(self):\r\n",
    "        return (self.TP + self.TN)/(self.TP + self.TN + self.FP + self.FN)\r\n",
    "        \r\n",
    "    def precision(self):\r\n",
    "        return (self.TP)/(self.TP + self.FP)\r\n",
    "\r\n",
    "    def recall(self):\r\n",
    "        return (self.TP)/(self.TP + self.FN)\r\n",
    "\r\n",
    "    def f1(self):\r\n",
    "        return (2 * self.precision() * self.recall())/(self.precision() + self.recall())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### sklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import classification_report\r\n",
    "print(classification_report(y_test, yhat=None))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Confusion matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from Lab03-02-NBM\r\n",
    "\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "mat = confusion_matrix(y_test, yhat)\r\n",
    "\r\n",
    "sns.heatmap(mat.T, annot=True, fmt=\"d\",\r\n",
    "           xticklabels=data.target_names, yticklabels=data.target_names)\r\n",
    "plt.xlabel('true')\r\n",
    "plt.ylabel('predicted')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Linear regression [Regression]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from time import time\r\n",
    "\r\n",
    "class LinearRegression:\r\n",
    "    def __init__(self, method=\"batch\", max_iter=10000, \r\n",
    "            tol=0.001, alpha=0.0001, batch_size=10):\r\n",
    "        self.method = method\r\n",
    "        self.max_iter = max_iter\r\n",
    "        self.tol = tol\r\n",
    "        self.alpha = alpha\r\n",
    "        self.batch_size = batch_size\r\n",
    "\r\n",
    "    def fit(self, X_train, y_train):\r\n",
    "        assert len(X_train)  == len(y_train)\r\n",
    "        assert len(X_test) == len(y_test)\r\n",
    "        loss_old = 10000\r\n",
    "        self.iter_stop = 0\r\n",
    "        self.theta = np.zeros(X_train.shape[1])\r\n",
    "        start = time()\r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"batch\":\r\n",
    "                self.X_train = X_train\r\n",
    "                self.y_train = y_train\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(0, X.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                self.X_train = X[ix, :].reshape(1, -1)\r\n",
    "                self.y_train = y[ix]\r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]: list_of_used_ix = []\r\n",
    "            elif self.method == \"mini\":\r\n",
    "                ix = np.random.randint(0, X.shape[0])\r\n",
    "                self.X_train = X[ix:ix+self.batch_size]\r\n",
    "                self.y_train = y[ix:ix+self.batch_size]\r\n",
    "            else:\r\n",
    "                print(\"method is not correct\")\r\n",
    "                break\r\n",
    "            \r\n",
    "            yhat = self.hx(self.X_train, self.theta)\r\n",
    "            error = yhat - self.y_train\r\n",
    "            grad = self.gradient(self.X_train, error)\r\n",
    "\r\n",
    "            if i>0 and i<4:\r\n",
    "                pass\r\n",
    "                #print(yhat.shape, self.y_train.shape)\r\n",
    "\r\n",
    "            self.theta = self.theta - self.alpha * grad\r\n",
    "\r\n",
    "            loss_new = self.MSE(yhat, self.y_train)\r\n",
    "            diff = abs(loss_new - loss_old)\r\n",
    "    \r\n",
    "            self.iter_stop = i+1\r\n",
    "            if diff < self.tol:\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                loss_old = loss_new\r\n",
    "        self.time_taken = time() - start\r\n",
    "\r\n",
    "    def evalute(self, X_test, y_test):\r\n",
    "        yhat_test = self.hx(X_test, self.theta)\r\n",
    "        mse = self.MSE(yhat_test, y_test)\r\n",
    "        return mse\r\n",
    "\r\n",
    "    def iter_stop(self):\r\n",
    "        return self.iter_stop\r\n",
    "\r\n",
    "    def time_taken(self):\r\n",
    "        return self.time_take\r\n",
    "\r\n",
    "    def hx(self, X, theta):\r\n",
    "        return X @ theta\r\n",
    "\r\n",
    "    def MSE(self, yhat, y):\r\n",
    "        return (((yhat - y)**2).sum()) / yhat.shape[0]\r\n",
    "\r\n",
    "    def gradient(self, X, error):\r\n",
    "        return X.T @ error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Logistic regression [Classification]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Class Logistic Regression\r\n",
    "class LogisticRegression:\r\n",
    "    def __init__(self, method=\"minibatch\", l_rate=0.01, \r\n",
    "                    batch_percent=10, max_iter=1000):\r\n",
    "        self.method = method\r\n",
    "        self.l_rate = l_rate\r\n",
    "        self.batch_percent = batch_percent\r\n",
    "        self.max_iter = max_iter\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.w = np.zeros(X.shape[1])\r\n",
    "        batch_size = int(self.batch_percent/100 * X.shape[0])\r\n",
    "        self.loss = []\r\n",
    "        list_of_used_ix = []     \r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"minibatch\":\r\n",
    "                ix = np.random.randint(0, X.shape[0])\r\n",
    "                batch_X = X[ix:ix+batch_size]\r\n",
    "                batch_y = y[ix:ix+batch_size]\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(0, X.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                batch_X = X[ix, :].reshape(1, -1)\r\n",
    "                batch_y = y[ix]\r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]: list_of_used_ix = []\r\n",
    "            elif self.method == \"batch\":\r\n",
    "                batch_X = X\r\n",
    "                batch_y = y\r\n",
    "            else:\r\n",
    "                print(\"Method is not match\")\r\n",
    "            cost, grad = self.gradient(batch_X, batch_y)\r\n",
    "            self.loss.append(cost)\r\n",
    "            self.w = self.w - self.l_rate * grad\r\n",
    "        self.iter = i+1\r\n",
    "        self.yhat = self.y_predict(X_test)\r\n",
    "\r\n",
    "    def sigmoid(self, x):        \r\n",
    "        return 1 / (1 + np.exp(-x))\r\n",
    "\r\n",
    "    def gradient(self, X, y):\r\n",
    "        h = self.h_theta(X, self.w)\r\n",
    "        error = h - y\r\n",
    "        # putting negative sign for negative log likelihood\r\n",
    "        cost = - np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\r\n",
    "        grad = np.dot(X.T, error)\r\n",
    "        return cost, grad\r\n",
    "\r\n",
    "    def h_theta(self, X, w):\r\n",
    "        return self.sigmoid(X @ w)\r\n",
    "\r\n",
    "    def plot_loss(self):\r\n",
    "        x_axis = [*range(self.iter)]\r\n",
    "        y_axis = self.loss\r\n",
    "        plt.plot(x_axis, y_axis)\r\n",
    "        plt.title(\"Losses - iteration\")\r\n",
    "        plt.xlabel(\"Iteration\")\r\n",
    "        plt.ylabel(\"Losses\")\r\n",
    "\r\n",
    "    def y_predict(self, X):\r\n",
    "        return np.round(self.h_theta(X, self.w))\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = LogisticRegression()\r\n",
    "model.fit(X_train, y_train)\r\n",
    "model.plot_loss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multinomial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding y\r\n",
    "k = len(set(y))\r\n",
    "m,n = X_train.shape\r\n",
    "Y_train_encoded = np.zeros((m, k))\r\n",
    "for each_class in range(k):\r\n",
    "    cond = Y_train==each_class\r\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LogisticRegression:\r\n",
    "    def __init__(self, method=\"minibatch\", max_iter=10000, l_rate=0.001, batch_size_ratio=0.1):\r\n",
    "        if (method != \"minibatch\") & (method != \"batch\") & (method != \"sto\"):\r\n",
    "            raise ValueError(\"Method is not match\")\r\n",
    "        else:\r\n",
    "            self.method = method\r\n",
    "            self.max_iter = max_iter\r\n",
    "            self.l_rate = l_rate\r\n",
    "            self.batch_size_ratio = batch_size_ratio\r\n",
    "\r\n",
    "    def fit(self, X, Y):\r\n",
    "        m = X.shape[0]\r\n",
    "        n = X.shape[1]\r\n",
    "        k = Y.shape[1]\r\n",
    "        self.W = np.random.rand(n, k)\r\n",
    "        batch_size = round(self.batch_size_ratio*m)\r\n",
    "        self.losses = []\r\n",
    "        list_of_used_ix = []\r\n",
    "        start = time()\r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"minibatch\":\r\n",
    "                idx = np.random.randint(0, m-batch_size)\r\n",
    "                X_batch = X[idx:idx+batch_size]\r\n",
    "                Y_batch = Y[idx:idx+batch_size]\r\n",
    "            elif self.method == \"batch\":\r\n",
    "                X_batch = X\r\n",
    "                Y_batch = Y\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(X_train.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                X_batch = X[idx, :].reshape(1, -1)\r\n",
    "                Y_batch = Y_train_encoded[idx]                \r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]:\r\n",
    "                    list_of_used_ix = []\r\n",
    "            cost, grad =  self.gradient(X_batch, Y_batch)\r\n",
    "            self.losses.append(cost)\r\n",
    "            self.W = self.W - self.l_rate * grad\r\n",
    "        self.runtime = time()-start\r\n",
    "\r\n",
    "    def gradient(self, X, Y):\r\n",
    "        m = X.shape[0]\r\n",
    "        h = self.h_theta(X, self.W)\r\n",
    "        cost = - np.sum(Y * np.log(h)) / m\r\n",
    "        error = h - Y\r\n",
    "        grad = self.softmax_grad(X, error)\r\n",
    "        return cost, grad\r\n",
    "\r\n",
    "    def softmax_grad(self, X, error):\r\n",
    "        return  X.T @ error\r\n",
    "            \r\n",
    "    def softmax(self, theta_t_x):\r\n",
    "        return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\r\n",
    "\r\n",
    "    def h_theta(self, X, W):\r\n",
    "        return self.softmax(X @ W)\r\n",
    "    \r\n",
    "    def predict(self, X):\r\n",
    "        return np.argmax(self.h_theta(X, self.W), axis=1)\r\n",
    "\r\n",
    "    def plot_losses(self):\r\n",
    "        x_axis = [*range(len(self.losses))]\r\n",
    "        y_axis = self.losses\r\n",
    "        plt.plot(x_axis, y_axis)\r\n",
    "        title = \"Losses - iteration \" + \"(\"+self.method+\")\"\r\n",
    "        plt.title(title)\r\n",
    "        plt.xlabel(\"Iteration\")\r\n",
    "        plt.ylabel(\"Losses\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Naive Bayesian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayesian - Gaussian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GaussianNaive:\r\n",
    "    def fit(self, X, y):\r\n",
    "        n = X.shape[1]\r\n",
    "        self.k =len(np.unique(y))\r\n",
    "        self.mean = np.zeros((self.k, n))\r\n",
    "        self.std = np.zeros((self.k, n))\r\n",
    "        m = np.zeros(self.k)\r\n",
    "        for label in range(self.k):\r\n",
    "            self.mean[label, :] = X[y==label].mean(axis=0)\r\n",
    "            self.std[label, :]  = X[y==label].std(axis=0)\r\n",
    "            m[label] = len(X[y==label])\r\n",
    "        self.prior = m/sum(m)\r\n",
    "\r\n",
    "    def gaussian_pdf(self, X, mean, std):\r\n",
    "        left = 1 / (np.sqrt(2 * np.pi) * std)\r\n",
    "        e = (X - mean) ** 2 / (2 * (std ** 2))\r\n",
    "        right = np.exp(-e)\r\n",
    "        return left*right\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        posterior = np.zeros((X.shape[0], self.k))\r\n",
    "        for label in range(self.k):\r\n",
    "            likelihood = self.gaussian_pdf(X, self.mean[label,:], self.std[label,:])\r\n",
    "            total_likelihood = np.prod(likelihood, axis=1)\r\n",
    "            posterior[:,label] = self.prior[label]*total_likelihood\r\n",
    "        yhat = np.argmax(posterior, axis=1)\r\n",
    "        return yhat\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayesian - Multinomial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n",
    "\r\n",
    "class NBM:\r\n",
    "    def __init__(self, vectorizer='count', laplace=1):\r\n",
    "        self.laplace = laplace\r\n",
    "        self.vectorizer = vectorizer\r\n",
    "\r\n",
    "    def transform(self, X_train, X_test, method):\r\n",
    "        if method != 'count' and method != 'Tfid':\r\n",
    "            raise ValueError(\"Method is not 'count' or 'Tfid'\")\r\n",
    "        v = CountVectorizer()\r\n",
    "        X_train = v.fit_transform(X_train)\r\n",
    "        X_test = v.transform(X_test)\r\n",
    "        if method == 'Tfid':\r\n",
    "            print(\"Tfid\")\r\n",
    "            v = TfidfTransformer()\r\n",
    "            X_train = v.fit_transform(X_train)\r\n",
    "            X_test = v.transform(X_test)\r\n",
    "        return X_train, X_test.toarray()\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        # Shape        \r\n",
    "        m, n = X.shape\r\n",
    "        self.classes = np.unique(y)\r\n",
    "        k = len(self.classes)\r\n",
    "        # fit\r\n",
    "        self.likelihoods = np.zeros((k,n))\r\n",
    "        self.priors = np.zeros(k)\r\n",
    "        for idx, label in enumerate(self.classes):\r\n",
    "            X_classed = X[y==label]\r\n",
    "            self.likelihoods[idx,:] = self.likelihood_fn(X_classed)\r\n",
    "            self.priors[idx] = self.prior_fn(y, label)\r\n",
    "\r\n",
    "    def likelihood_fn(self, X_class):\r\n",
    "        dividend  = ((X_class.sum(axis=0)) + self.laplace)\r\n",
    "        devider = (np.sum(X_class.sum(axis=0) + self.laplace))\r\n",
    "        return  dividend/devider\r\n",
    "\r\n",
    "    def prior_fn(self, y, label):\r\n",
    "        return len(y[y==label])/len(y)\r\n",
    "\r\n",
    "    def predict(self, X_test):\r\n",
    "        yhat = np.log(self.priors) + X_test @ np.log(self.likelihoods.T)\r\n",
    "        yhat = np.argmax(yhat, axis=1)\r\n",
    "        return yhat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cvxopt\r\n",
    "\r\n",
    "# Kernel\r\n",
    "def linear(x, z):\r\n",
    "    return np.dot(x, z.T)\r\n",
    "\r\n",
    "def polynomial(x, z, p=5):\r\n",
    "    return (1 + np.dot(x, z.T)) ** p\r\n",
    "\r\n",
    "def gaussian(x, z, sigma=0.9999):\r\n",
    "    return np.exp(-np.linalg.norm(x - z, axis=1) ** 2 / (2 * (sigma ** 2)))\r\n",
    "\r\n",
    "# SVM\r\n",
    "class SVM:\r\n",
    "    def __init__(self, kernel=gaussian, C=1):\r\n",
    "        self.kernel = kernel\r\n",
    "        self.C = C\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.y = y\r\n",
    "        self.X = X\r\n",
    "        m, n = X.shape\r\n",
    "\r\n",
    "        # Calculate Kernel\r\n",
    "        self.K = np.zeros((m, m))\r\n",
    "        for i in range(m):\r\n",
    "            self.K[i, :] = self.kernel(X[i, np.newaxis], self.X)\r\n",
    "\r\n",
    "        # Solve with cvxopt final QP needs to be reformulated\r\n",
    "        # to match the input form for cvxopt.solvers.qp\r\n",
    "        P = cvxopt.matrix(np.outer(y, y) * self.K)\r\n",
    "        q = cvxopt.matrix(-np.ones((m, 1)))\r\n",
    "        G = cvxopt.matrix(np.vstack((np.eye(m) * -1, np.eye(m))))\r\n",
    "        h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * self.C)))\r\n",
    "        A = cvxopt.matrix(y, (1, m), \"d\")\r\n",
    "        b = cvxopt.matrix(np.zeros(1))\r\n",
    "        cvxopt.solvers.options[\"show_progress\"] = False\r\n",
    "        sol = cvxopt.solvers.qp(P, q, G, h, A, b)\r\n",
    "        self.alphas = np.array(sol[\"x\"])\r\n",
    "\r\n",
    "    def predict(self, X):  #<----this is X_test\r\n",
    "        y_predict = np.zeros((X.shape[0]))\r\n",
    "        sv = self.get_parameters(self.alphas)\r\n",
    "\r\n",
    "        for i in range(X.shape[0]):\r\n",
    "            y_predict[i] = np.sum(\r\n",
    "                self.alphas[sv]\r\n",
    "                * self.y[sv, np.newaxis]\r\n",
    "                * self.kernel(X[i], self.X[sv])[:, np.newaxis]\r\n",
    "            )\r\n",
    "\r\n",
    "        return np.sign(y_predict + self.b)\r\n",
    "\r\n",
    "    def get_parameters(self, alphas):\r\n",
    "        threshold = 1e-5\r\n",
    "\r\n",
    "        sv = ((alphas > threshold) * (alphas < self.C)).flatten()\r\n",
    "        self.w = np.dot(self.X[sv].T, alphas[sv] * self.y[sv, np.newaxis])\r\n",
    "        self.b = np.mean(\r\n",
    "            self.y[sv, np.newaxis]\r\n",
    "            - self.alphas[sv] * self.y[sv, np.newaxis] * self.K[sv, sv][:, np.newaxis]\r\n",
    "        )\r\n",
    "        return sv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. K-Nearest Neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class KNN:\r\n",
    "    def predict(self, X_train, X_test, y_train, k=3):\r\n",
    "        classes = len(np.unique(y_train))\r\n",
    "        neighbors_ix = self.find_neighbors(X_train, X_test, k)\r\n",
    "\r\n",
    "        pred = np.zeros(X_test.shape[0])\r\n",
    "        prob = np.zeros((X_test.shape[0]))\r\n",
    "        for ix, y in enumerate(y_train[neighbors_ix]):\r\n",
    "            freq = np.bincount(y)\r\n",
    "            while len(freq) < classes:\r\n",
    "                freq = np.append(freq, 0)\r\n",
    "            k_inc = k\r\n",
    "            while np.sort(freq)[-1] == np.sort(freq)[-2]:\r\n",
    "                k_inc += 1\r\n",
    "                neighbors_ix_new = self.find_neighbors(X_train, X_test[ix].reshape(1,-1), k_inc).reshape(-1)\r\n",
    "                freq = np.bincount(y_train[neighbors_ix_new])\r\n",
    "                while len(freq) < classes:\r\n",
    "                    freq = np.append(freq, 0)\r\n",
    "            pred[ix] = self.get_most_common(y)\r\n",
    "            prob_all = freq/np.sum(freq)\r\n",
    "            prob[ix] = prob_all[int(pred[ix])]\r\n",
    "        return pred, prob\r\n",
    "\r\n",
    "    def find_distance(self, X_train, X_test):\r\n",
    "        #create newaxis simply so that broadcast to all values\r\n",
    "        dist = X_test[:, np.newaxis, :] - X_train[np.newaxis, :, :]\r\n",
    "        sq_dist = dist ** 2\r\n",
    "\r\n",
    "        #sum across feature dimension, thus axis = 2\r\n",
    "        summed_dist = sq_dist.sum(axis=2)\r\n",
    "        sq_dist = np.sqrt(summed_dist)\r\n",
    "        return sq_dist\r\n",
    "\r\n",
    "    def find_neighbors(self, X_train, X_test, k=3):\r\n",
    "        dist = self.find_distance(X_train, X_test)\r\n",
    "        #return the first k neighbors\r\n",
    "        neighbors_ix = np.argsort(dist)[:, 0:k]\r\n",
    "        return neighbors_ix\r\n",
    "\r\n",
    "    def get_most_common(self, y):\r\n",
    "        return np.bincount(y).argmax()\r\n",
    "\r\n",
    "    def CV_K(self, X_train_val, y_train_val, K_max=5, cv=3):\r\n",
    "        # Split train data and validation data\r\n",
    "        m, n = X_train_val.shape\r\n",
    "        idx = list(range(m))\r\n",
    "        idx_List = []\r\n",
    "        for i in range(cv):\r\n",
    "            idx_List.append(idx[i*int(m/cv):(i+1)*int(m/cv)])\r\n",
    "        # Predict and find accuracy\r\n",
    "        acc = []\r\n",
    "        K = []\r\n",
    "        for i in range(1, K_max+1):\r\n",
    "            acc_sum = 0\r\n",
    "            for idx in idx_List:\r\n",
    "                X_val = X_train_val[idx]\r\n",
    "                y_val = y_train_val[idx]\r\n",
    "                X_train = np.delete(X_train_val,idx, axis=0)\r\n",
    "                y_train = np.delete(y_train_val,idx, axis=0)\r\n",
    "                yhat, yhat_prob = self.predict(X_train, X_val, y_train, k=i)\r\n",
    "                acc_sum += np.sum(yhat == y_val)/len(y_val)\r\n",
    "            acc.append(acc_sum/cv)\r\n",
    "            K.append(i)\r\n",
    "        return acc, K"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = KNN()\r\n",
    "acc, K = model.CV_K(X_train, y_train, K_max=10, cv=5)\r\n",
    "idx = np.argmax(acc)\r\n",
    "print(\"Best K:\", K[idx], \"Accuracy:\",acc[idx])\r\n",
    "plt.plot(K, acc)\r\n",
    "plt.xlabel(\"K\")\r\n",
    "plt.ylabel(\"Accuracy\")\r\n",
    "plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Decision Trees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. AdaBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Gradient Boosting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. K-Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. GMM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}