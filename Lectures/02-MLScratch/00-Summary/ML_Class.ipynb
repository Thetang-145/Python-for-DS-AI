{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Library import"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Linear regression [Regression]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from time import time\r\n",
    "\r\n",
    "class LinearRegression:\r\n",
    "    def __init__(self, method=\"batch\", max_iter=10000, \r\n",
    "            tol=0.001, alpha=0.0001, batch_size=10):\r\n",
    "        self.method = method\r\n",
    "        self.max_iter = max_iter\r\n",
    "        self.tol = tol\r\n",
    "        self.alpha = alpha\r\n",
    "        self.batch_size = batch_size\r\n",
    "\r\n",
    "    def fit(self, X_train, y_train):\r\n",
    "        assert len(X_train)  == len(y_train)\r\n",
    "        assert len(X_test) == len(y_test)\r\n",
    "        loss_old = 10000\r\n",
    "        self.iter_stop = 0\r\n",
    "        self.theta = np.zeros(X_train.shape[1])\r\n",
    "        start = time()\r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"batch\":\r\n",
    "                self.X_train = X_train\r\n",
    "                self.y_train = y_train\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(0, X.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                self.X_train = X[ix, :].reshape(1, -1)\r\n",
    "                self.y_train = y[ix]\r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]: list_of_used_ix = []\r\n",
    "            elif self.method == \"mini\":\r\n",
    "                ix = np.random.randint(0, X.shape[0])\r\n",
    "                self.X_train = X[ix:ix+self.batch_size]\r\n",
    "                self.y_train = y[ix:ix+self.batch_size]\r\n",
    "            else:\r\n",
    "                print(\"method is not correct\")\r\n",
    "                break\r\n",
    "            \r\n",
    "            yhat = self.hx(self.X_train, self.theta)\r\n",
    "            error = yhat - self.y_train\r\n",
    "            grad = self.gradient(self.X_train, error)\r\n",
    "\r\n",
    "            if i>0 and i<4:\r\n",
    "                pass\r\n",
    "                #print(yhat.shape, self.y_train.shape)\r\n",
    "\r\n",
    "            self.theta = self.theta - self.alpha * grad\r\n",
    "\r\n",
    "            loss_new = self.MSE(yhat, self.y_train)\r\n",
    "            diff = abs(loss_new - loss_old)\r\n",
    "    \r\n",
    "            self.iter_stop = i+1\r\n",
    "            if diff < self.tol:\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                loss_old = loss_new\r\n",
    "        self.time_taken = time() - start\r\n",
    "\r\n",
    "    def evalute(self, X_test, y_test):\r\n",
    "        yhat_test = self.hx(X_test, self.theta)\r\n",
    "        mse = self.MSE(yhat_test, y_test)\r\n",
    "        return mse\r\n",
    "\r\n",
    "    def iter_stop(self):\r\n",
    "        return self.iter_stop\r\n",
    "\r\n",
    "    def time_taken(self):\r\n",
    "        return self.time_take\r\n",
    "\r\n",
    "    def hx(self, X, theta):\r\n",
    "        return X @ theta\r\n",
    "\r\n",
    "    def MSE(self, yhat, y):\r\n",
    "        return (((yhat - y)**2).sum()) / yhat.shape[0]\r\n",
    "\r\n",
    "    def gradient(self, X, error):\r\n",
    "        return X.T @ error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Logistic regression [Classification]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Class Logistic Regression\r\n",
    "class LogisticRegression:\r\n",
    "    def __init__(self, method=\"minibatch\", l_rate=0.01, \r\n",
    "                    batch_percent=10, max_iter=1000):\r\n",
    "        self.method = method\r\n",
    "        self.l_rate = l_rate\r\n",
    "        self.batch_percent = batch_percent\r\n",
    "        self.max_iter = max_iter\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.w = np.zeros(X.shape[1])\r\n",
    "        batch_size = int(self.batch_percent/100 * X.shape[0])\r\n",
    "        self.loss = []\r\n",
    "        list_of_used_ix = []     \r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"minibatch\":\r\n",
    "                ix = np.random.randint(0, X.shape[0])\r\n",
    "                batch_X = X[ix:ix+batch_size]\r\n",
    "                batch_y = y[ix:ix+batch_size]\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(0, X.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                batch_X = X[ix, :].reshape(1, -1)\r\n",
    "                batch_y = y[ix]\r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]: list_of_used_ix = []\r\n",
    "            elif self.method == \"batch\":\r\n",
    "                batch_X = X\r\n",
    "                batch_y = y\r\n",
    "            else:\r\n",
    "                print(\"Method is not match\")\r\n",
    "            cost, grad = self.gradient(batch_X, batch_y)\r\n",
    "            self.loss.append(cost)\r\n",
    "            self.w = self.w - self.l_rate * grad\r\n",
    "        self.iter = i+1\r\n",
    "        self.yhat = self.y_predict(X_test)\r\n",
    "\r\n",
    "    def sigmoid(self, x):        \r\n",
    "        return 1 / (1 + np.exp(-x))\r\n",
    "\r\n",
    "    def gradient(self, X, y):\r\n",
    "        h = self.h_theta(X, self.w)\r\n",
    "        error = h - y\r\n",
    "        # putting negative sign for negative log likelihood\r\n",
    "        cost = - np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\r\n",
    "        grad = np.dot(X.T, error)\r\n",
    "        return cost, grad\r\n",
    "\r\n",
    "    def h_theta(self, X, w):\r\n",
    "        return self.sigmoid(X @ w)\r\n",
    "\r\n",
    "    def plot_loss(self):\r\n",
    "        x_axis = [*range(self.iter)]\r\n",
    "        y_axis = self.loss\r\n",
    "        plt.plot(x_axis, y_axis)\r\n",
    "        plt.title(\"Losses - iteration\")\r\n",
    "        plt.xlabel(\"Iteration\")\r\n",
    "        plt.ylabel(\"Losses\")\r\n",
    "\r\n",
    "    def y_predict(self, X):\r\n",
    "        return np.round(self.h_theta(X, self.w))\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = LogisticRegression()\r\n",
    "model.fit(X_train, y_train)\r\n",
    "model.plot_loss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Multinomial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encoding y\r\n",
    "k = len(set(y))\r\n",
    "m,n = X_train.shape\r\n",
    "Y_train_encoded = np.zeros((m, k))\r\n",
    "for each_class in range(k):\r\n",
    "    cond = Y_train==each_class\r\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LogisticRegression:\r\n",
    "    def __init__(self, method=\"minibatch\", max_iter=10000, l_rate=0.001, batch_size_ratio=0.1):\r\n",
    "        if (method != \"minibatch\") & (method != \"batch\") & (method != \"sto\"):\r\n",
    "            raise ValueError(\"Method is not match\")\r\n",
    "        else:\r\n",
    "            self.method = method\r\n",
    "            self.max_iter = max_iter\r\n",
    "            self.l_rate = l_rate\r\n",
    "            self.batch_size_ratio = batch_size_ratio\r\n",
    "\r\n",
    "    def fit(self, X, Y):\r\n",
    "        m = X.shape[0]\r\n",
    "        n = X.shape[1]\r\n",
    "        k = Y.shape[1]\r\n",
    "        self.W = np.random.rand(n, k)\r\n",
    "        batch_size = round(self.batch_size_ratio*m)\r\n",
    "        self.losses = []\r\n",
    "        list_of_used_ix = []\r\n",
    "        start = time()\r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"minibatch\":\r\n",
    "                idx = np.random.randint(0, m-batch_size)\r\n",
    "                X_batch = X[idx:idx+batch_size]\r\n",
    "                Y_batch = Y[idx:idx+batch_size]\r\n",
    "            elif self.method == \"batch\":\r\n",
    "                X_batch = X\r\n",
    "                Y_batch = Y\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(X_train.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                X_batch = X[idx, :].reshape(1, -1)\r\n",
    "                Y_batch = Y_train_encoded[idx]                \r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]:\r\n",
    "                    list_of_used_ix = []\r\n",
    "            cost, grad =  self.gradient(X_batch, Y_batch)\r\n",
    "            self.losses.append(cost)\r\n",
    "            self.W = self.W - self.l_rate * grad\r\n",
    "        self.runtime = time()-start\r\n",
    "\r\n",
    "    def gradient(self, X, Y):\r\n",
    "        m = X.shape[0]\r\n",
    "        h = self.h_theta(X, self.W)\r\n",
    "        cost = - np.sum(Y * np.log(h)) / m\r\n",
    "        error = h - Y\r\n",
    "        grad = self.softmax_grad(X, error)\r\n",
    "        return cost, grad\r\n",
    "\r\n",
    "    def softmax_grad(self, X, error):\r\n",
    "        return  X.T @ error\r\n",
    "            \r\n",
    "    def softmax(self, theta_t_x):\r\n",
    "        return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\r\n",
    "\r\n",
    "    def h_theta(self, X, W):\r\n",
    "        return self.softmax(X @ W)\r\n",
    "    \r\n",
    "    def predict(self, X):\r\n",
    "        return np.argmax(self.h_theta(X, self.W), axis=1)\r\n",
    "\r\n",
    "    def plot_losses(self):\r\n",
    "        x_axis = [*range(len(self.losses))]\r\n",
    "        y_axis = self.losses\r\n",
    "        plt.plot(x_axis, y_axis)\r\n",
    "        title = \"Losses - iteration \" + \"(\"+self.method+\")\"\r\n",
    "        plt.title(title)\r\n",
    "        plt.xlabel(\"Iteration\")\r\n",
    "        plt.ylabel(\"Losses\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Naive Bayesian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayesian - Gaussian"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GaussianNaive:\r\n",
    "    def fit(self, X, y):\r\n",
    "        n = X.shape[1]\r\n",
    "        self.k =len(np.unique(y))\r\n",
    "        self.mean = np.zeros((self.k, n))\r\n",
    "        self.std = np.zeros((self.k, n))\r\n",
    "        m = np.zeros(self.k)\r\n",
    "        for label in range(self.k):\r\n",
    "            self.mean[label, :] = X[y==label].mean(axis=0)\r\n",
    "            self.std[label, :]  = X[y==label].std(axis=0)\r\n",
    "            m[label] = len(X[y==label])\r\n",
    "        self.prior = m/sum(m)\r\n",
    "\r\n",
    "    def gaussian_pdf(self, X, mean, std):\r\n",
    "        left = 1 / (np.sqrt(2 * np.pi) * std)\r\n",
    "        e = (X - mean) ** 2 / (2 * (std ** 2))\r\n",
    "        right = np.exp(-e)\r\n",
    "        return left*right\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        posterior = np.zeros((X.shape[0], self.k))\r\n",
    "        for label in range(self.k):\r\n",
    "            likelihood = self.gaussian_pdf(X, self.mean[label,:], self.std[label,:])\r\n",
    "            total_likelihood = np.prod(likelihood, axis=1)\r\n",
    "            posterior[:,label] = self.prior[label]*total_likelihood\r\n",
    "        yhat = np.argmax(posterior, axis=1)\r\n",
    "        return yhat\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayesian - Multinomial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\r\n",
    "\r\n",
    "class NBM:\r\n",
    "    def __init__(self, vectorizer='count', laplace=1):\r\n",
    "        self.laplace = laplace\r\n",
    "        self.vectorizer = vectorizer\r\n",
    "\r\n",
    "    def transform(self, X_train, X_test, method):\r\n",
    "        if method != 'count' and method != 'Tfid':\r\n",
    "            raise ValueError(\"Method is not 'count' or 'Tfid'\")\r\n",
    "        v = CountVectorizer()\r\n",
    "        X_train = v.fit_transform(X_train)\r\n",
    "        X_test = v.transform(X_test)\r\n",
    "        if method == 'Tfid':\r\n",
    "            print(\"Tfid\")\r\n",
    "            v = TfidfTransformer()\r\n",
    "            X_train = v.fit_transform(X_train)\r\n",
    "            X_test = v.transform(X_test)\r\n",
    "        return X_train, X_test.toarray()\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        # Shape        \r\n",
    "        m, n = X.shape\r\n",
    "        self.classes = np.unique(y)\r\n",
    "        k = len(self.classes)\r\n",
    "        # fit\r\n",
    "        self.likelihoods = np.zeros((k,n))\r\n",
    "        self.priors = np.zeros(k)\r\n",
    "        for idx, label in enumerate(self.classes):\r\n",
    "            X_classed = X[y==label]\r\n",
    "            self.likelihoods[idx,:] = self.likelihood_fn(X_classed)\r\n",
    "            self.priors[idx] = self.prior_fn(y, label)\r\n",
    "\r\n",
    "    def likelihood_fn(self, X_class):\r\n",
    "        dividend  = ((X_class.sum(axis=0)) + self.laplace)\r\n",
    "        devider = (np.sum(X_class.sum(axis=0) + self.laplace))\r\n",
    "        return  dividend/devider\r\n",
    "\r\n",
    "    def prior_fn(self, y, label):\r\n",
    "        return len(y[y==label])/len(y)\r\n",
    "\r\n",
    "    def predict(self, X_test):\r\n",
    "        yhat = np.log(self.priors) + X_test @ np.log(self.likelihoods.T)\r\n",
    "        yhat = np.argmax(yhat, axis=1)\r\n",
    "        return yhat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cvxopt\r\n",
    "\r\n",
    "# Kernel\r\n",
    "def linear(x, z):\r\n",
    "    return np.dot(x, z.T)\r\n",
    "\r\n",
    "def polynomial(x, z, p=5):\r\n",
    "    return (1 + np.dot(x, z.T)) ** p\r\n",
    "\r\n",
    "def gaussian(x, z, sigma=0.9999):\r\n",
    "    return np.exp(-np.linalg.norm(x - z, axis=1) ** 2 / (2 * (sigma ** 2)))\r\n",
    "\r\n",
    "# SVM\r\n",
    "class SVM:\r\n",
    "    def __init__(self, kernel=gaussian, C=1):\r\n",
    "        self.kernel = kernel\r\n",
    "        self.C = C\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.y = y\r\n",
    "        self.X = X\r\n",
    "        m, n = X.shape\r\n",
    "\r\n",
    "        # Calculate Kernel\r\n",
    "        self.K = np.zeros((m, m))\r\n",
    "        for i in range(m):\r\n",
    "            self.K[i, :] = self.kernel(X[i, np.newaxis], self.X)\r\n",
    "\r\n",
    "        # Solve with cvxopt final QP needs to be reformulated\r\n",
    "        # to match the input form for cvxopt.solvers.qp\r\n",
    "        P = cvxopt.matrix(np.outer(y, y) * self.K)\r\n",
    "        q = cvxopt.matrix(-np.ones((m, 1)))\r\n",
    "        G = cvxopt.matrix(np.vstack((np.eye(m) * -1, np.eye(m))))\r\n",
    "        h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * self.C)))\r\n",
    "        A = cvxopt.matrix(y, (1, m), \"d\")\r\n",
    "        b = cvxopt.matrix(np.zeros(1))\r\n",
    "        cvxopt.solvers.options[\"show_progress\"] = False\r\n",
    "        sol = cvxopt.solvers.qp(P, q, G, h, A, b)\r\n",
    "        self.alphas = np.array(sol[\"x\"])\r\n",
    "\r\n",
    "    def predict(self, X):  #<----this is X_test\r\n",
    "        y_predict = np.zeros((X.shape[0]))\r\n",
    "        sv = self.get_parameters(self.alphas)\r\n",
    "\r\n",
    "        for i in range(X.shape[0]):\r\n",
    "            y_predict[i] = np.sum(\r\n",
    "                self.alphas[sv]\r\n",
    "                * self.y[sv, np.newaxis]\r\n",
    "                * self.kernel(X[i], self.X[sv])[:, np.newaxis]\r\n",
    "            )\r\n",
    "\r\n",
    "        return np.sign(y_predict + self.b)\r\n",
    "\r\n",
    "    def get_parameters(self, alphas):\r\n",
    "        threshold = 1e-5\r\n",
    "\r\n",
    "        sv = ((alphas > threshold) * (alphas < self.C)).flatten()\r\n",
    "        self.w = np.dot(self.X[sv].T, alphas[sv] * self.y[sv, np.newaxis])\r\n",
    "        self.b = np.mean(\r\n",
    "            self.y[sv, np.newaxis]\r\n",
    "            - self.alphas[sv] * self.y[sv, np.newaxis] * self.K[sv, sv][:, np.newaxis]\r\n",
    "        )\r\n",
    "        return sv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. K-Nearest Neighbors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class KNN:\r\n",
    "    def predict(self, X_train, X_test, y_train, k=3):\r\n",
    "        classes = len(np.unique(y_train))\r\n",
    "        neighbors_ix = self.find_neighbors(X_train, X_test, k)\r\n",
    "\r\n",
    "        pred = np.zeros(X_test.shape[0])\r\n",
    "        prob = np.zeros((X_test.shape[0]))\r\n",
    "        for ix, y in enumerate(y_train[neighbors_ix]):\r\n",
    "            freq = np.bincount(y)\r\n",
    "            while len(freq) < classes:\r\n",
    "                freq = np.append(freq, 0)\r\n",
    "            k_inc = k\r\n",
    "            while np.sort(freq)[-1] == np.sort(freq)[-2]:\r\n",
    "                k_inc += 1\r\n",
    "                neighbors_ix_new = self.find_neighbors(X_train, X_test[ix].reshape(1,-1), k_inc).reshape(-1)\r\n",
    "                freq = np.bincount(y_train[neighbors_ix_new])\r\n",
    "                while len(freq) < classes:\r\n",
    "                    freq = np.append(freq, 0)\r\n",
    "            pred[ix] = self.get_most_common(y)\r\n",
    "            prob_all = freq/np.sum(freq)\r\n",
    "            prob[ix] = prob_all[int(pred[ix])]\r\n",
    "        return pred, prob\r\n",
    "\r\n",
    "    def find_distance(self, X_train, X_test):\r\n",
    "        #create newaxis simply so that broadcast to all values\r\n",
    "        dist = X_test[:, np.newaxis, :] - X_train[np.newaxis, :, :]\r\n",
    "        sq_dist = dist ** 2\r\n",
    "\r\n",
    "        #sum across feature dimension, thus axis = 2\r\n",
    "        summed_dist = sq_dist.sum(axis=2)\r\n",
    "        sq_dist = np.sqrt(summed_dist)\r\n",
    "        return sq_dist\r\n",
    "\r\n",
    "    def find_neighbors(self, X_train, X_test, k=3):\r\n",
    "        dist = self.find_distance(X_train, X_test)\r\n",
    "        #return the first k neighbors\r\n",
    "        neighbors_ix = np.argsort(dist)[:, 0:k]\r\n",
    "        return neighbors_ix\r\n",
    "\r\n",
    "    def get_most_common(self, y):\r\n",
    "        return np.bincount(y).argmax()\r\n",
    "\r\n",
    "    def CV_K(self, X_train_val, y_train_val, K_max=5, cv=3):\r\n",
    "        # Split train data and validation data\r\n",
    "        m, n = X_train_val.shape\r\n",
    "        idx = list(range(m))\r\n",
    "        idx_List = []\r\n",
    "        for i in range(cv):\r\n",
    "            idx_List.append(idx[i*int(m/cv):(i+1)*int(m/cv)])\r\n",
    "        # Predict and find accuracy\r\n",
    "        acc = []\r\n",
    "        K = []\r\n",
    "        for i in range(1, K_max+1):\r\n",
    "            acc_sum = 0\r\n",
    "            for idx in idx_List:\r\n",
    "                X_val = X_train_val[idx]\r\n",
    "                y_val = y_train_val[idx]\r\n",
    "                X_train = np.delete(X_train_val,idx, axis=0)\r\n",
    "                y_train = np.delete(y_train_val,idx, axis=0)\r\n",
    "                yhat, yhat_prob = self.predict(X_train, X_val, y_train, k=i)\r\n",
    "                acc_sum += np.sum(yhat == y_val)/len(y_val)\r\n",
    "            acc.append(acc_sum/cv)\r\n",
    "            K.append(i)\r\n",
    "        return acc, K"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = KNN()\r\n",
    "acc, K = model.CV_K(X_train, y_train, K_max=10, cv=5)\r\n",
    "idx = np.argmax(acc)\r\n",
    "print(\"Best K:\", K[idx], \"Accuracy:\",acc[idx])\r\n",
    "plt.plot(K, acc)\r\n",
    "plt.xlabel(\"K\")\r\n",
    "plt.ylabel(\"Accuracy\")\r\n",
    "plt.show"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Decision Trees"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Node:\r\n",
    "    def __init__(self, predicted_class):\r\n",
    "        self.predicted_class = predicted_class\r\n",
    "        self.feature_index = 0\r\n",
    "        self.threshold = 0\r\n",
    "        self.left = None\r\n",
    "        self.right = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DecisionTree:\r\n",
    "    def __init__(self , max_depth=5):\r\n",
    "        self.max_depth = max_depth\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.n_classes_ = len(set(y))\r\n",
    "        self.n_features_ = X.shape[1]\r\n",
    "        self.node = self.grow_tree(X, y, self.n_classes_)\r\n",
    "\r\n",
    "    def grow_tree(self, Xtrain, ytrain, n_classes, depth=0):\r\n",
    "        num_samples_per_class = [np.sum(ytrain == i) for i in range(n_classes)]\r\n",
    "        #predicted class using the majority of sample class\r\n",
    "        predicted_class = np.argmax(num_samples_per_class)\r\n",
    "        \r\n",
    "        #define the parent node\r\n",
    "        node = Node(predicted_class = predicted_class)\r\n",
    "        #perform recursion\r\n",
    "        if depth < self.max_depth:\r\n",
    "            feature, threshold = self.find_split(Xtrain, ytrain, n_classes)\r\n",
    "            if feature is not None:\r\n",
    "                #take all the indices that is less than threshold\r\n",
    "                indices_left = Xtrain[:, feature] < threshold\r\n",
    "                X_left, y_left = Xtrain[indices_left], ytrain[indices_left]\r\n",
    "                #tilde for negation\r\n",
    "                X_right, y_right = Xtrain[~indices_left], ytrain[~indices_left]\r\n",
    "                #take note for later decision\r\n",
    "                node.feature_index = feature\r\n",
    "                node.threshold = threshold\r\n",
    "                node.left = self.grow_tree(X_left, y_left, n_classes, depth + 1)\r\n",
    "                node.right = self.grow_tree(X_right, y_right, n_classes, depth + 1)\r\n",
    "        return node\r\n",
    "        \r\n",
    "    def find_split(self, X, y, n_classes):\r\n",
    "        \"\"\" Find split where children has lowest impurity possible\r\n",
    "        in condition where the purity should also be less than the parent,\r\n",
    "        if not, stop.\r\n",
    "        \"\"\"\r\n",
    "        n_samples, n_features = X.shape\r\n",
    "        if n_samples <= 1:\r\n",
    "            return None, None\r\n",
    "        \r\n",
    "        #so it will not have any warning about \"referenced before assignments\"\r\n",
    "        feature_ix, threshold = None, None\r\n",
    "        \r\n",
    "        # Count of each class in the current node.\r\n",
    "        sample_per_class_parent = [np.sum(y == c) for c in range(n_classes)] #[2, 2]\r\n",
    "        \r\n",
    "        # Gini of parent node.\r\n",
    "        best_gini = 1.0 - sum((n / n_samples) ** 2 for n in sample_per_class_parent)\r\n",
    "\r\n",
    "        # Loop through all features.\r\n",
    "        for feature in range(n_features):\r\n",
    "            \r\n",
    "            # Sort data along selected feature.\r\n",
    "            sample_sorted = sorted(X[:, feature]) #[2, 3, 10, 19]\r\n",
    "            sort_idx = np.argsort(X[:, feature])\r\n",
    "            y_sorted = y[sort_idx] #[0, 0, 1, 1]\r\n",
    "                    \r\n",
    "            sample_per_class_left = [0] * n_classes   #[0, 0]\r\n",
    "            \r\n",
    "            sample_per_class_right = sample_per_class_parent.copy() #[2, 2]\r\n",
    "\r\n",
    "            #sample_sorted, y_sorted = zip(*sorted(zip(X[:, i], y)))\r\n",
    "            #loop through each threshold, 2.5, 6.5, 14.5\r\n",
    "            #1st iter: [-] [-++]\r\n",
    "            #2nd iter: [--] [++]\r\n",
    "            #3rd iter: [--+] [+]\r\n",
    "            for i in range(1, n_samples): #1 to 3 (excluding 4)\r\n",
    "                #the class of that sample\r\n",
    "                c = y_sorted[i - 1]  #[0]\r\n",
    "                \r\n",
    "                #put the sample to the left\r\n",
    "                sample_per_class_left[c] += 1  #[1, 0]\r\n",
    "                            \r\n",
    "                #take the sample out from the right  [1, 2]\r\n",
    "                sample_per_class_right[c] -= 1\r\n",
    "                \r\n",
    "                gini_left = 1.0 - sum(\r\n",
    "                    (sample_per_class_left[x] / i) ** 2 for x in range(n_classes)\r\n",
    "                )\r\n",
    "                            \r\n",
    "                #we divided by n_samples - i since we know that the left amount of samples\r\n",
    "                #since left side has already i samples\r\n",
    "                gini_right = 1.0 - sum(\r\n",
    "                    (sample_per_class_right[x] / (n_samples - i)) ** 2 for x in range(n_classes)\r\n",
    "                )\r\n",
    "\r\n",
    "                #weighted gini\r\n",
    "                weighted_gini = ((i / n_samples) * gini_left) + ( (n_samples - i) /n_samples) * gini_right\r\n",
    "\r\n",
    "                # in case the value are the same, we do not split\r\n",
    "                # (both have to end up on the same side of a split).\r\n",
    "                if sample_sorted[i] == sample_sorted[i - 1]:\r\n",
    "                    continue\r\n",
    "\r\n",
    "                if weighted_gini < best_gini:\r\n",
    "                    best_gini = weighted_gini\r\n",
    "                    feature_ix = feature\r\n",
    "                    threshold = (sample_sorted[i] + sample_sorted[i - 1]) / 2  # midpoint\r\n",
    "\r\n",
    "        #return the feature number and threshold \r\n",
    "        #used to find best split\r\n",
    "        return feature_ix, threshold\r\n",
    "\r\n",
    "    def predict(self, sample):\r\n",
    "        tree = self.node\r\n",
    "        for sample_i in sample:\r\n",
    "            while tree.left:\r\n",
    "                if sample_i[tree.feature_index] < tree.threshold:\r\n",
    "                    tree = tree.left\r\n",
    "                else:\r\n",
    "                    tree = tree.right\r\n",
    "        return tree.predicted_class"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "from sklearn.metrics import classification_report, accuracy_score\r\n",
    "from scipy import stats\r\n",
    "import random\r\n",
    "\r\n",
    "class Bagging:\r\n",
    "    def __init__(self, B=5, boostrap_ratio=0.8, wo_rpm=True, max_features='sqrt'):\r\n",
    "        self.B = B\r\n",
    "        self.boostrap_ratio = boostrap_ratio\r\n",
    "        self.wo_rpm = wo_rpm\r\n",
    "        self.max_features = max_features\r\n",
    "\r\n",
    "    def fit(self, X_train, y_train):\r\n",
    "        m, n = X_train.shape\r\n",
    "        tree_params = {\r\n",
    "            'max_depth': 2, \r\n",
    "            'criterion':'gini', \r\n",
    "            'max_features': self.max_features}\r\n",
    "        self.models = [DecisionTreeClassifier(**tree_params) for _ in range(self.B)]\r\n",
    "        sample_size = int(self.boostrap_ratio * len(X_train))\r\n",
    "        xsamples = np.zeros((self.B, sample_size, n))\r\n",
    "        ysamples = np.zeros((self.B, sample_size))\r\n",
    "        # subsamples for each model\r\n",
    "        x_oob = []\r\n",
    "        y_oob = []\r\n",
    "        idx_list = []\r\n",
    "        for i in range(self.B):\r\n",
    "            idx_list.append([])\r\n",
    "            for j in range(sample_size):\r\n",
    "                idx = random.randrange(m)\r\n",
    "                if (self.wo_rpm):\r\n",
    "                    while idx in idx_list[i]:\r\n",
    "                        idx = random.randrange(m)\r\n",
    "                idx_list[i].append(idx)\r\n",
    "                xsamples[i, j, :] = X_train[idx]\r\n",
    "                ysamples[i, j] = y_train[idx]\r\n",
    "            x_oob.append(np.delete(X_train, idx_list[i], axis=0))\r\n",
    "            y_oob.append(np.delete(y_train, idx_list[i], axis=0))\r\n",
    "        # fit each model\r\n",
    "        for i, model in enumerate(self.models):\r\n",
    "            _X = xsamples[i, :]\r\n",
    "            _y = ysamples[i, :]\r\n",
    "            model.fit(_X, _y)\r\n",
    "        # find the average score of OOB\r\n",
    "        acc = np.zeros(self.B)\r\n",
    "        for i in range(self.B):\r\n",
    "            yhat = self.models[i].predict(x_oob[i])\r\n",
    "            acc[i]=(accuracy_score(y_oob[i], yhat))\r\n",
    "            print(\"Tree\", i, \":\", acc[i])\r\n",
    "        avg_score = np.average(acc)\r\n",
    "        print(\"Average score of OOB =\",avg_score)\r\n",
    "\r\n",
    "    def predict(self, X_test):\r\n",
    "        predictions = np.zeros((self.B, X_test.shape[0]))\r\n",
    "        for i, model in enumerate(self.models):\r\n",
    "            yhat = model.predict(X_test)\r\n",
    "            predictions[i, :] = yhat\r\n",
    "\r\n",
    "        yhat = stats.mode(predictions)[0][0]\r\n",
    "        return yhat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. AdaBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Strump:\r\n",
    "    def __init__(self):\r\n",
    "        self.polarity = 1\r\n",
    "        self.feature_index = None\r\n",
    "        self.threshold = None\r\n",
    "        self.alpha = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\r\n",
    "\r\n",
    "class Adaboost:\r\n",
    "    def __init__(self, eta = 0.5, S = 20):\r\n",
    "        self.eta = eta\r\n",
    "        self.S = S\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        m, n = X.shape\r\n",
    "\r\n",
    "        W = np.full(m, 1/m)\r\n",
    "\r\n",
    "        self.clfs = []\r\n",
    "\r\n",
    "        for i in range(self.S):\r\n",
    "            model = Strump()\r\n",
    "            min_err = 1\r\n",
    "            for feature in range(n):\r\n",
    "                X_sorted = np.sort(X[:, feature])\r\n",
    "                thd_list = (X_sorted[:-1]+X_sorted[1:])/2\r\n",
    "                for thd in thd_list:\r\n",
    "                    for polarity in [1, -1]:\r\n",
    "                        yhat = np.ones(m)\r\n",
    "                        yhat[polarity*X[:,feature] < polarity*thd] = -1\r\n",
    "                        \r\n",
    "                        err = W[(yhat != y)].sum()\r\n",
    "                        if err < min_err:\r\n",
    "                            model.polarity = polarity\r\n",
    "                            model.threshold = thd\r\n",
    "                            model.feature_index = feature\r\n",
    "                            yhat_best = yhat\r\n",
    "                            min_err = err\r\n",
    "                            \r\n",
    "            # Give min limit of err to be the lowest value of float\r\n",
    "            min_err = max(min_err,sys.float_info.min)\r\n",
    "            model.alpha = np.log ((1 - min_err) / min_err) * self.eta\r\n",
    "            W = (W * np.exp(model.alpha * y * yhat_best)) \r\n",
    "            W = W / sum (W)\r\n",
    "            self.clfs.append(model)\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        m = X.shape[0]\r\n",
    "        yhat = np.zeros(m)\r\n",
    "        for model in self.clfs:\r\n",
    "            h = np.ones(m)\r\n",
    "            h[model.polarity*X[:,model.feature_index] < model.polarity*model.threshold] = -1\r\n",
    "            yhat += model.alpha * h\r\n",
    "        return np.sign(yhat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Gradient Boosting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.dummy import DummyRegressor\r\n",
    "\r\n",
    "class Gradient_Boosting:\r\n",
    "    def __init__(self, S=100, learning_rate=0.1, max_depth=1, min_samples_split=2, regression=True):\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.regression = regression\r\n",
    "        self.first_model = DummyRegressor(strategy='mean')\r\n",
    "        tree_params = {'max_depth': max_depth, 'min_samples_split': min_samples_split}\r\n",
    "        self.models = [DecisionTreeRegressor(**tree_params) for _ in range(S)]\r\n",
    "\r\n",
    "    def grad(self, y, h):\r\n",
    "        return y - h\r\n",
    "\r\n",
    "    def encode(self, y):\r\n",
    "        classes = np.unique(y)\r\n",
    "        y_encode = np.zeros((len(y), len(classes)))\r\n",
    "        for label in range(len(classes)):\r\n",
    "            y_encode[np.where(y==label), label] = 1\r\n",
    "        return y_encode\r\n",
    "    \r\n",
    "    def fit(self, X, y):\r\n",
    "        if not self.regression and len(y.shape) == 1:\r\n",
    "            y = self.encode(y)\r\n",
    "        \r\n",
    "        self.first_model.fit(X, y)\r\n",
    "        self.models_trained = [self.first_model]\r\n",
    "        self.i = 0\r\n",
    "        #fit the estimators\r\n",
    "        for model in self.models:\r\n",
    "            y_pred = self.predict(X, Argmax=False)\r\n",
    "            \r\n",
    "            #errors will be the total errors maded by models_trained\r\n",
    "            residual = self.grad(y, y_pred)\r\n",
    "            \r\n",
    "            #fit the next model with residual\r\n",
    "            self.i += 1\r\n",
    "            #print(self.i, X.shape, residual.shape, y_pred.shape)\r\n",
    "            model.fit(X, residual)\r\n",
    "            \r\n",
    "            self.models_trained.append(model)\r\n",
    "\r\n",
    "    def predict(self, X, Argmax=True):\r\n",
    "        #print('X',X.shape)\r\n",
    "        models = self.models_trained\r\n",
    "        f0 = models[0].predict(X)  #first use the dummy model\r\n",
    "        boosting = sum(self.learning_rate * model.predict(X) for model in models[1:])\r\n",
    "        y_pred = (f0 + boosting)\r\n",
    "        if not self.regression:\r\n",
    "            #print(models[0], X.shape, y_pred.shape, y_pred[0])\r\n",
    "            y_pred = np.exp(y_pred) / np.sum(np.exp(y_pred), axis=1, keepdims=True)\r\n",
    "            if Argmax:\r\n",
    "                y_pred = np.argmax(y_pred, axis=1)\r\n",
    "        return y_pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. K-Means"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\r\n",
    "from time import time\r\n",
    "\r\n",
    "class kmeans:\r\n",
    "    def __init__(self, precent_batch=50, tol=1e-2, max_iter=10000):\r\n",
    "        self.precent_batch = precent_batch\r\n",
    "        self.tol = tol\r\n",
    "        self.max_iter = max_iter\r\n",
    "\r\n",
    "    def plot_K_variation(self, X, K_max):\r\n",
    "        variation_list = []\r\n",
    "        K_list = range(2,K_max+1)\r\n",
    "        for K in K_list:\r\n",
    "            self.fit(X, K)\r\n",
    "            print('K:', K, ', Number of iteration:', self.iteration, ', Variation', self.variation)\r\n",
    "            variation_list.append(self.variation)\r\n",
    "        plt.plot(K_list, variation_list)\r\n",
    "        plt.xlabel('K')\r\n",
    "        plt.ylabel('Variation')\r\n",
    "    \r\n",
    "    def fit(self, X, n_clusters):\r\n",
    "        m, n = X.shape\r\n",
    "\r\n",
    "        #1. randomly choose n clusters from X\r\n",
    "        #you can also randomly generate any two points\r\n",
    "        rng = np.random.RandomState(42)\r\n",
    "        i = rng.permutation(m)[:n_clusters]\r\n",
    "        self.centers = X[i]\r\n",
    "\r\n",
    "        for iter in range(self.max_iter):\r\n",
    "            batch_size = int(self.precent_batch*m/100)\r\n",
    "            batch_idx = np.random.permutation(m)\r\n",
    "            X_batch = X[batch_idx[:batch_size]]\r\n",
    "            #2. assign lables based on closest center\r\n",
    "            #return the index of centers having smallest\r\n",
    "            #distance with X\r\n",
    "            labels = pairwise_distances_argmin(X_batch, self.centers)\r\n",
    "\r\n",
    "            #3. find new centers\r\n",
    "            new_centers = []\r\n",
    "            for i in range(n_clusters):\r\n",
    "                new_centers.append(X_batch[labels == i].mean(axis=0))\r\n",
    "\r\n",
    "            #convert list to np.array; you can actually combine #3\r\n",
    "            #with np.array in one sentence \r\n",
    "            new_centers = np.array(new_centers)\r\n",
    "\r\n",
    "            #4 stopping criteria - if centers do not \r\n",
    "            #change anymore, we stop!\r\n",
    "            if(np.allclose(self.centers, new_centers, rtol=self.tol)):\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                self.centers = new_centers\r\n",
    "        self.iteration = iter+1\r\n",
    "        self.variation = 0\r\n",
    "        labels = pairwise_distances_argmin(X, self.centers)\r\n",
    "        for i in range(n_clusters):\r\n",
    "            self.variation += np.sum((X[labels==i]-np.mean(X[labels==i],axis=0))**2)\r\n",
    "            \r\n",
    "    def predict(self, X):\r\n",
    "        return pairwise_distances_argmin(X, self.centers)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11. GMM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.stats import multivariate_normal\r\n",
    "\r\n",
    "class GMM:\r\n",
    "    def __init__(self, max_iter=50, early_stop=1):\r\n",
    "        self.max_iter = max_iter\r\n",
    "        self.early_stop = early_stop\r\n",
    "\r\n",
    "    def fit(self, X, K=3):\r\n",
    "        m, n = X.shape\r\n",
    "        #==initialization==\r\n",
    "        #responsibliity\r\n",
    "        self.r = np.full(shape=(m, K), fill_value=1/K)\r\n",
    "        #pi\r\n",
    "        pi = np.full((K, ), fill_value=1/K) #simply use 1/k for pi\r\n",
    "        #mean\r\n",
    "        random_row = np.random.randint(low=0, high=m, size=K)\r\n",
    "        mean = np.array([X[idx,:] for idx in random_row ]).T #.T to make to shape (M, K)\r\n",
    "        #covariance\r\n",
    "        cov = np.array([np.cov(X.T) for _ in range (K)])\r\n",
    "        \r\n",
    "        prevNLL = 0\r\n",
    "        for iteration in range(self.max_iter):\r\n",
    "            \r\n",
    "            #===E-Step=====\r\n",
    "            #Update r_ik of each sample\r\n",
    "            NLL = 0\r\n",
    "            for i in range(m):\r\n",
    "                for k in range(K):\r\n",
    "                    xi_pdf = multivariate_normal.pdf(X[i], mean=mean[:, k], cov=cov[k])\r\n",
    "                    self.r[i, k] = pi[k] * xi_pdf\r\n",
    "                    NLL += np.log(pi[k])+xi_pdf\r\n",
    "                self.r[i] /= np.sum(self.r[i])\r\n",
    "\r\n",
    "            # Early stop\r\n",
    "            if np.abs(prevNLL-NLL) < self.early_stop:\r\n",
    "                print(\"Early stop at iteration:\", iteration)\r\n",
    "                break\r\n",
    "            prevNLL=NLL\r\n",
    "\r\n",
    "            # Plot cluster\r\n",
    "            if (iteration+1)%5==0:\r\n",
    "                yhat = np.argmax(self.r, axis=1)\r\n",
    "                plt.figure()\r\n",
    "                plt.scatter(X[:, 0], X[:, 1], c=yhat)\r\n",
    "                plt.title(\"Iteration:\"+ str(iteration+1))\r\n",
    "                plt.show()\r\n",
    "\r\n",
    "            #===M-Step====\r\n",
    "            # Find NK first for latter use\r\n",
    "            NK = np.sum(self.r, axis=0)\r\n",
    "            assert NK.shape == (K, )\r\n",
    "            \r\n",
    "            #PI\r\n",
    "            pi = NK / m\r\n",
    "            assert pi.shape == (K, )\r\n",
    "            \r\n",
    "            #mean\r\n",
    "            mean =  ( X.T @ self.r ) / NK\r\n",
    "            assert mean.shape == (n, K)\r\n",
    "            \r\n",
    "            #covariance (also called Sigma)\r\n",
    "            cov = np.zeros((K, n, n))\r\n",
    "            for k in range(K):\r\n",
    "                for i in range(m):\r\n",
    "                    X_mean = (X[i]-mean[:, k]).reshape(-1, 1)\r\n",
    "                    cov[k] += self.r[i, k] * (X_mean @ X_mean.T)\r\n",
    "                cov[k] /= NK[k]\r\n",
    "            assert cov.shape == (K, n, n)\r\n",
    "\r\n",
    "    def get_cluster(self, plot_result=True):\r\n",
    "        yhat = np.argmax(self.r, axis=1)\r\n",
    "        if plot_result:\r\n",
    "            plt.figure()\r\n",
    "            plt.scatter(X[:, 0], X[:, 1], c=yhat)\r\n",
    "            plt.title(\"Final cluster\")\r\n",
    "            plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}