{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### ===Task===\n",
    "\n",
    "#### Out of Bag Evaluation\n",
    "\n",
    "Well, it seems like our bagging technique is quite good.  Anyhow, one interesting observation is that each tree only see a subset of the dataset. Any data that a particular tree did not see is called **out of bag** (oob).  Note that oob is not the same for all predictors.\n",
    "\n",
    "One interesting thing is that since oob is something that each tree never see, thus oob is somewhat a validation set.  Thus what we can do is after we fit each tree. We can ask each tree to test their accuracy with their own oob, and then we can average the accuracy from all trees.  \n",
    "\n",
    "Let's modify the above scratch code to:\n",
    "- Calculate for oob evaluation for each bootstrapped dataset, and also the average score\n",
    "- Change the code to \"without replacement\"\n",
    "- Put everything into a class <code>Bagging</code>.  It should have at least two methods, <code>fit(X_train, y_train)</code>, and <code>predict(X_test)</code>\n",
    "- Modify the code from above to randomize features.  Set the number of features to be used in each tree to be <code>sqrt(n)</code>, and then select a subset of features for each tree.  This can be easily done by setting our DecisionTreeClassifier <code>max_features</code> to 'sqrt'"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "iris = load_iris()\r\n",
    "X = iris.data\r\n",
    "y = iris.target\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
    "                test_size=0.3, shuffle=True, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\r\n",
    "import random\r\n",
    "from scipy import stats\r\n",
    "from sklearn.metrics import classification_report, accuracy_score\r\n",
    "\r\n",
    "B = 5\r\n",
    "m, n = X_train.shape\r\n",
    "boostrap_ratio = 0.8\r\n",
    "tree_params = {'max_depth': 2, 'criterion':'gini', 'min_samples_split': 5}\r\n",
    "models = [DecisionTreeClassifier(**tree_params) for _ in range(B)]\r\n",
    "\r\n",
    "#sample size for each tree\r\n",
    "sample_size = int(boostrap_ratio * len(X_train))\r\n",
    "\r\n",
    "xsamples = np.zeros((B, sample_size, n))\r\n",
    "ysamples = np.zeros((B, sample_size))\r\n",
    "\r\n",
    "x_oob = []\r\n",
    "y_oob = []\r\n",
    "\r\n",
    "wo_rpm = False\r\n",
    "idx_list = []\r\n",
    "#subsamples for each model\r\n",
    "for i in range(B):\r\n",
    "    ##sampling with replacement; i.e., sample can occur more than once\r\n",
    "    #for the same predictor\r\n",
    "    idx_list.append([])\r\n",
    "    for j in range(sample_size):\r\n",
    "        idx = random.randrange(m)   #<----with replacement #change so no repetition\r\n",
    "        if wo_rpm:\r\n",
    "            while idx in idx_list[i]:\r\n",
    "                idx = random.randrange(m)\r\n",
    "        idx_list[i].append(idx)\r\n",
    "        xsamples[i, j, :] = X_train[idx]\r\n",
    "        ysamples[i, j] = y_train[idx]\r\n",
    "        #keep track of idx that i did not use for ith tree\r\n",
    "    x_oob.append(np.delete(X_train, idx_list[i], axis=0))\r\n",
    "    y_oob.append(np.delete(y_train, idx_list[i], axis=0))\r\n",
    "\r\n",
    "#fitting each estimator\r\n",
    "for i, model in enumerate(models):\r\n",
    "    _X = xsamples[i, :]\r\n",
    "    _y = ysamples[i, :]\r\n",
    "    model.fit(_X, _y)\r\n",
    "\r\n",
    "predictions = []\r\n",
    "acc = np.zeros(B)\r\n",
    "for i in range(B):\r\n",
    "    yhat = models[i].predict(x_oob[i])\r\n",
    "    predictions.append(yhat)\r\n",
    "    acc[i]=(accuracy_score(y_oob[i], yhat))\r\n",
    "avg_score = np.average(acc)\r\n",
    "print(avg_score)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9031884057971015\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "class Bagging:\r\n",
    "    def __init__(self, B=5, boostrap_ratio=0.8, wo_rpm=True, max_features='sqrt'):\r\n",
    "        self.B = B\r\n",
    "        self.boostrap_ratio = boostrap_ratio\r\n",
    "        self.wo_rpm = wo_rpm\r\n",
    "        self.max_features = max_features\r\n",
    "\r\n",
    "    def fit(self, X_train, y_train):\r\n",
    "        m, n = X_train.shape\r\n",
    "        tree_params = {\r\n",
    "            'max_depth': 2, \r\n",
    "            'criterion':'gini', \r\n",
    "            'max_features': self.max_features}\r\n",
    "        self.models = [DecisionTreeClassifier(**tree_params) for _ in range(self.B)]\r\n",
    "        sample_size = int(self.boostrap_ratio * len(X_train))\r\n",
    "        xsamples = np.zeros((B, sample_size, n))\r\n",
    "        ysamples = np.zeros((B, sample_size))\r\n",
    "        # subsamples for each model\r\n",
    "        x_oob = []\r\n",
    "        y_oob = []\r\n",
    "        idx_list = []\r\n",
    "        for i in range(B):\r\n",
    "            idx_list.append([])\r\n",
    "            for j in range(sample_size):\r\n",
    "                idx = random.randrange(m)\r\n",
    "                if (self.wo_rpm):\r\n",
    "                    while idx in idx_list[i]:\r\n",
    "                        idx = random.randrange(m)\r\n",
    "                idx_list[i].append(idx)\r\n",
    "                xsamples[i, j, :] = X_train[idx]\r\n",
    "                ysamples[i, j] = y_train[idx]\r\n",
    "            x_oob.append(np.delete(X_train, idx_list[i], axis=0))\r\n",
    "            y_oob.append(np.delete(y_train, idx_list[i], axis=0))\r\n",
    "        # fit each model\r\n",
    "        for i, model in enumerate(self.models):\r\n",
    "            _X = xsamples[i, :]\r\n",
    "            _y = ysamples[i, :]\r\n",
    "            model.fit(_X, _y)\r\n",
    "        # find the average score of OOB\r\n",
    "        acc = np.zeros(B)\r\n",
    "        for i in range(B):\r\n",
    "            yhat = self.models[i].predict(x_oob[i])\r\n",
    "            acc[i]=(accuracy_score(y_oob[i], yhat))\r\n",
    "            print(\"Tree\", i, \":\", acc[i])\r\n",
    "        avg_score = np.average(acc)\r\n",
    "        print(\"Average score of OOB =\",avg_score)\r\n",
    "\r\n",
    "    def predict(self, X_test):\r\n",
    "        predictions = np.zeros((B, X_test.shape[0]))\r\n",
    "        for i, model in enumerate(self.models):\r\n",
    "            yhat = model.predict(X_test)\r\n",
    "            predictions[i, :] = yhat\r\n",
    "\r\n",
    "        yhat = stats.mode(predictions)[0][0]\r\n",
    "        return yhat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "model = Bagging(B=5, boostrap_ratio=0.8, wo_rpm=True, max_features='sqrt')\r\n",
    "model.fit(X_train, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tree 0 : 0.9047619047619048\n",
      "Tree 1 : 0.9523809523809523\n",
      "Tree 2 : 0.9523809523809523\n",
      "Tree 3 : 0.9523809523809523\n",
      "Tree 4 : 0.8571428571428571\n",
      "Average score of OOB = 0.9238095238095237\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "yhat = model.predict(X_test)\r\n",
    "print(classification_report(y_test, yhat))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        19\n",
      "           1       1.00      1.00      1.00        13\n",
      "           2       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}