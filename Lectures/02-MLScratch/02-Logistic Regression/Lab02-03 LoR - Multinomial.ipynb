{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### === Task ===\r\n",
    "\r\n",
    "1. With the iris data given in class, implement train_test_split from scratch.\r\n",
    "\r\n",
    "2. Put everything into a class called LogisticRegression, this class should allow you choose any of the training methods you'd like including \"batch\", \"minibatch\" and \"sto\". However, if the input method is not one of the three, it should \"raise ValueError\".\r\n",
    "\r\n",
    "3. Calculate time taken to fit your models using different training methods.\r\n",
    "\r\n",
    "4. Perform a classification on the dataset using all 3 methods and also show what happens if your defined training method is not either \"batch\", \"minibatch\" or \"sto\". Make sure to plot the training losses.\r\n",
    "\r\n",
    "5. Simply, use classification_report from sklearn.metrics to evaluate your models.\r\n",
    "\r\n",
    "6. Discuss your results ie. training losses of the three methods and time taken to fit models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from sklearn import datasets\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from time import time\r\n",
    "\r\n",
    "# import some data\r\n",
    "iris = datasets.load_iris()\r\n",
    "X = iris.data[:, 2:]\r\n",
    "y = iris.target\r\n",
    "\r\n",
    "# feature scaling\r\n",
    "scaler = StandardScaler()\r\n",
    "X = scaler.fit_transform(X)\r\n",
    "\r\n",
    "# data split\r\n",
    "train_size = round(0.7 * X.shape[0])\r\n",
    "X_train = X[:train_size, :]\r\n",
    "Y_train = y[:train_size]\r\n",
    "X_test = X[train_size:,:]\r\n",
    "Y_test = y[train_size:]\r\n",
    "\r\n",
    "print(X_train.shape)\r\n",
    "print(Y_train.shape)\r\n",
    "print(X_test.shape)\r\n",
    "print(Y_test.shape)\r\n",
    "\r\n",
    "# add intercept to our X\r\n",
    "intercept = np.ones((X_train.shape[0], 1))\r\n",
    "X_train   = np.concatenate((intercept, X_train), axis=1)\r\n",
    "intercept = np.ones((X_test.shape[0], 1))\r\n",
    "X_test    = np.concatenate((intercept, X_test), axis=1)\r\n",
    "\r\n",
    "k = len(set(y))\r\n",
    "m = X_train.shape[0]\r\n",
    "n = X_train.shape[1]\r\n",
    "Y_train_encoded = np.zeros((m, k))\r\n",
    "for each_class in range(k):\r\n",
    "    cond = Y_train==each_class\r\n",
    "    Y_train_encoded[np.where(cond), each_class] = 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(105, 2)\n",
      "(105,)\n",
      "(45, 2)\n",
      "(45,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "class LogisticRegression:\r\n",
    "    def __init__(self, method=\"minibatch\", max_iter=1000, l_rate=0.01, batch_size_ratio=0.1):\r\n",
    "        if (method != \"minibatch\") & (method != \"batch\") & (method != \"sto\"):\r\n",
    "            raise ValueError(\"Method is not match\")\r\n",
    "        else:\r\n",
    "            self.method = method\r\n",
    "            self.max_iter = max_iter\r\n",
    "            self.l_rate = l_rate\r\n",
    "            self.batch_size_ratio = batch_size_ratio\r\n",
    "\r\n",
    "    def fit(self, X, Y):\r\n",
    "        m = X.shape[0]\r\n",
    "        n = X.shape[1]\r\n",
    "        k = Y.shape[1]\r\n",
    "        start = time()\r\n",
    "        self.W = np.random.rand(n, k)\r\n",
    "        batch_size = round(self.batch_size_ratio*m)\r\n",
    "        list_of_used_ix = []\r\n",
    "        for i in range(self.max_iter):\r\n",
    "            if self.method == \"minibatch\":\r\n",
    "                idx = np.random.randint(0, m-batch_size)\r\n",
    "                X_batch = X[idx:idx+batch_size]\r\n",
    "                Y_batch = Y[idx:idx+batch_size]\r\n",
    "            elif self.method == \"batch\":\r\n",
    "                X_batch = X\r\n",
    "                Y_batch = Y\r\n",
    "            elif self.method == \"sto\":\r\n",
    "                idx = np.random.randint(X_train.shape[0])\r\n",
    "                while idx in list_of_used_ix:\r\n",
    "                    idx = np.random.randint(X_train.shape[0])\r\n",
    "                X_batch = X[idx, :].reshape(1, -1)\r\n",
    "                Y_batch = Y_train_encoded[idx]                \r\n",
    "                list_of_used_ix.append(idx)\r\n",
    "                if len(list_of_used_ix) == X_train.shape[0]:\r\n",
    "                    list_of_used_ix = []\r\n",
    "\r\n",
    "            cost, grad =  self.gradient(X_batch, Y_batch, self.W)\r\n",
    "            if i % 500 == 0:\r\n",
    "                print(f\"Cost at iteration {i}\", cost)\r\n",
    "            self.W = self.W - self.l_rate * grad\r\n",
    "        self.runtime = time()-start\r\n",
    "\r\n",
    "    def gradient(self, X, Y, W):\r\n",
    "        m = X.shape[0]\r\n",
    "        h = self.h_theta(X, W)\r\n",
    "        cost = - np.sum(Y * np.log(h)) / m\r\n",
    "        error = h - Y\r\n",
    "        grad = self.softmax_grad(X, error)\r\n",
    "        return cost, grad\r\n",
    "\r\n",
    "    def softmax_grad(self, X, error):\r\n",
    "        return  X.T @ error\r\n",
    "            \r\n",
    "    def softmax(self, theta_t_x):\r\n",
    "        return np.exp(theta_t_x) / np.sum(np.exp(theta_t_x), axis=1, keepdims=True)\r\n",
    "\r\n",
    "    def h_theta(self, X, W):\r\n",
    "        return self.softmax(X @ W)\r\n",
    "\r\n",
    "    def plot_losses(self):\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "model_mini = LogisticRegression(method=\"minibatch\")\r\n",
    "model_mini.fit(X_train, Y_train_encoded)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost at iteration 0 0.682254458275567\n",
      "Cost at iteration 500 0.020909133773342584\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "model_batch = LogisticRegression(method=\"batch\")\r\n",
    "model_batch.fit(X_train, Y_train_encoded)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost at iteration 0 0.9020057063563102\n",
      "Cost at iteration 500 0.029143148012840923\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "model_sto = LogisticRegression(method=\"sto\")\r\n",
    "model_sto.fit(X_train, Y_train_encoded)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost at iteration 0 0.7962857007453462\n",
      "Cost at iteration 500 0.2585207486279982\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "print(model_sto.runtime)\r\n",
    "print(model_mini.runtime)\r\n",
    "print(model_batch.runtime)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.04671430587768555\n",
      "0.0329127311706543\n",
      "0.03390955924987793\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}