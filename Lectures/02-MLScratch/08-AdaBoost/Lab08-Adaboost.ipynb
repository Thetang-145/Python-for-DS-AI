{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### ===Task===\r\n",
    "\r\n",
    "Your work: Let's modify the above scratch code:\r\n",
    "- Notice that if <code>err</code> = 0, then $\\alpha$ will be undefined, thus attempt to fix this by adding some very small value to the lower term\r\n",
    "- Notice that sklearn version of AdaBoost has a parameter <code>learning_rate</code>.  This is in fact the $\\frac{1}{2}$ in front of the $\\alpha$ calculation.  Attempt to change this $\\frac{1}{2}$ into a parameter called <code>eta</code>, and try different values of it and see whether accuracy is improved.  Note that sklearn default this value to 1.\r\n",
    "- Observe that we are actually using sklearn DecisionTreeClassifier.  If we take a look at it closely, it is actually using weighted gini index, instead of weighted errors that we learn above.   Attempt to write your own class of <code>class Stump</code> that actually uses weighted errors, instead of weighted gini index\r\n",
    "- Put everything into a class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.datasets import make_classification\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "X, y = make_classification(n_samples=500, random_state=1)\r\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    X, y, test_size=0.3, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class Strump:\r\n",
    "    def __init__(self):\r\n",
    "        self.polarity = 1\r\n",
    "        self.feature_index = None\r\n",
    "        self.threshold = None\r\n",
    "        self.alpha = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class Adaboost:\r\n",
    "    def __init__(self, eta = 0.5, S = 20):\r\n",
    "        self.eta = eta\r\n",
    "        self.S = S\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        m, n = X.shape\r\n",
    "\r\n",
    "        W = np.full(m, 1/m)\r\n",
    "\r\n",
    "        self.clfs = []\r\n",
    "\r\n",
    "        for i in range(self.S):\r\n",
    "            model = Strump()\r\n",
    "            min_err = 1\r\n",
    "            for feature in range(n):\r\n",
    "                X_sorted = np.sort(X[:, feature])\r\n",
    "                thd_list = (X_sorted[:-1]+X_sorted[1:])/2\r\n",
    "                for thd in thd_list:\r\n",
    "                    for polarity in [1, -1]:\r\n",
    "                        yhat = np.ones(m)\r\n",
    "                        yhat[polarity*X[:,feature] < polarity*thd] = -1\r\n",
    "                        # Give min limit of err to be the most min foalt\r\n",
    "                        err = max(W[(yhat != y)].sum().sum(),sys.float_info.min)\r\n",
    "                        if err < min_err:\r\n",
    "                            model.polarity = polarity\r\n",
    "                            model.threshold = thd\r\n",
    "                            model.feature_index = feature\r\n",
    "                            yhat_best = yhat\r\n",
    "                            min_err = err\r\n",
    "            model.alpha = np.log ((1 - err) / err) * self.eta\r\n",
    "            W = (W * np.exp(model.alpha * y * yhat_best)) \r\n",
    "            W = W / sum (W)\r\n",
    "            self.clfs.append(model)\r\n",
    "\r\n",
    "    def predict(self, X):\r\n",
    "        m = X.shape[0]\r\n",
    "        yhat = np.zeros(m)\r\n",
    "        for model in self.clfs:\r\n",
    "            h = np.ones(m)\r\n",
    "            h[model.polarity*X[:,model.feature_index] < model.polarity*model.threshold] = -1\r\n",
    "            yhat += model.alpha * h\r\n",
    "        return np.sign(yhat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from sklearn.datasets import make_classification\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "\r\n",
    "X, y = make_classification(n_samples=500, random_state=1)\r\n",
    "y = np.where(y==0,-1,1)  #change our y to be -1 if it is 0, otherwise 1\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    X, y, test_size=0.3, random_state=42)\r\n",
    "\r\n",
    "model = Adaboost(S=10)\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "print(classification_report(y_test, yhat))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.95      0.94        79\n",
      "           1       0.94      0.93      0.94        71\n",
      "\n",
      "    accuracy                           0.94       150\n",
      "   macro avg       0.94      0.94      0.94       150\n",
      "weighted avg       0.94      0.94      0.94       150\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}